<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
        <link rel="shortcut icon" href="/images/favicon32.ico">
    
    
        <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon16.ico">
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon32.ico">
    
    
    


    <!-- meta -->


<title>PINN-01-哈密顿神经网络（HNN） | 狮子大张嘴&#39;s blog</title>


    <meta name="keywords" content="machine learning, 经典力学">




    <!-- OpenGraph -->
 
    <meta name="description" content="本研快结了，有功夫看些杂七杂八的论文了。physics informed neural network（PINN）里细读的第一篇论文。很早就对hamiltonian neural network（NIPS 2019）感兴趣。体量不大，花了几个小时读了下，有点失望其实，感觉噱头大于实际。">
<meta property="og:type" content="article">
<meta property="og:title" content="PINN-01-哈密顿神经网络（HNN）">
<meta property="og:url" content="https://yawninglion.github.io/2022/05/08/post25/index.html">
<meta property="og:site_name" content="狮子大张嘴&#39;s blog">
<meta property="og:description" content="本研快结了，有功夫看些杂七杂八的论文了。physics informed neural network（PINN）里细读的第一篇论文。很早就对hamiltonian neural network（NIPS 2019）感兴趣。体量不大，花了几个小时读了下，有点失望其实，感觉噱头大于实际。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic1.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic2.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic3.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic4.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic5.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic3.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic6.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic7.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic8.png">
<meta property="og:image" content="https://yawninglion.github.io/2022/05/08/post25/pic9.png">
<meta property="article:published_time" content="2022-05-07T17:31:23.000Z">
<meta property="article:modified_time" content="2022-05-09T14:07:44.032Z">
<meta property="article:author" content="yawning-lion">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="经典力学">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yawninglion.github.io/2022/05/08/post25/pic1.png">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
        <link rel="stylesheet" id="hl-default-theme" href="/css/highlight/default.css" media="none" >
        
            <link rel="stylesheet" id="hl-dark-theme" href="/css/highlight/dark.css" media="none">
        
    

    
    

    
    
<link rel="stylesheet" href="/css/style/dark.css">

    
<script src="/js/darkmode.js"></script>



     

    <!-- custom head -->

<meta name="generator" content="Hexo 5.4.1"></head>

    <body>
        <div id="app">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">狮子大张嘴&#39;s blog</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">home</a>
                
                    <a href="/tags/" class="navbar-menu button">tags</a>
                
                    <a href="/categories" class="navbar-menu button">categories</a>
                
                    <a href="/archives/" class="navbar-menu button">archives</a>
                
                    <a href="/friends/" class="navbar-menu button">friends</a>
                
                    <a href="/page/" class="navbar-menu button">Page</a>
                
            </div>
        
        
        
    <a href="/search/" id="btn-search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="24" height="24" fill="currentColor" stroke="currentColor" stroke-width="32"><path d="M192 448c0-141.152 114.848-256 256-256s256 114.848 256 256-114.848 256-256 256-256-114.848-256-256z m710.624 409.376l-206.88-206.88A318.784 318.784 0 0 0 768 448c0-176.736-143.264-320-320-320S128 271.264 128 448s143.264 320 320 320a318.784 318.784 0 0 0 202.496-72.256l206.88 206.88 45.248-45.248z"></path></svg>
    </a>


        
        
    <a href="javaScript:void(0);" id="btn-toggle-dark">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
    </a>


        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">home</a>
                
                    <a href="/tags/" class="dropdown-menu button">tags</a>
                
                    <a href="/categories" class="dropdown-menu button">categories</a>
                
                    <a href="/archives/" class="dropdown-menu button">archives</a>
                
                    <a href="/friends/" class="dropdown-menu button">friends</a>
                
                    <a href="/page/" class="dropdown-menu button">Page</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        PINN-01-哈密顿神经网络（HNN）
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2022/05/" class="post-meta__date button">2022-05-08</a>
        
    <span class="separate-dot"></span><a href="/categories/%E6%AF%8F%E4%B8%80%E6%AD%A5%E8%B7%AF%E9%83%BD%E7%AE%97%E6%95%B0/" class="button">每一步路都算数</a>

 
        
    
    


 

 
    </div>
</div>


    <aside class="post-side">
        <div class="post-side__toc">
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%93%88%E5%AF%86%E9%A1%BF%E5%8A%9B%E5%AD%A6%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A"><span class="toc-text">哈密顿力学必知必会</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E6%96%B9%E7%A8%8B"><span class="toc-text">正则方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E7%A9%BA%E9%97%B4"><span class="toc-text">相空间</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HNN"><span class="toc-text">HNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#baseline-nn"><span class="toc-text">baseline nn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HNN-1"><span class="toc-text">HNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-text">实例分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%90%E6%8C%AF%E5%AD%90"><span class="toc-text">谐振子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%8E%B0"><span class="toc-text">网络表现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90"><span class="toc-text">数量分析</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%AD%E7%A7%98HNN"><span class="toc-text">揭秘HNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%86%E5%A4%A9%E7%9A%84total-HNN-conserved-quantity"><span class="toc-text">逆天的total HNN-conserved quantity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%B4%A8%E6%98%AF%E6%8A%97%E6%89%B0%E5%8A%A8%E8%83%BD%E5%8A%9B%EF%BC%88or%E4%BF%9D%E8%BE%9B%E6%80%A7%EF%BC%9F%EF%BC%89"><span class="toc-text">本质是抗扰动能力（or保辛性？）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E4%B8%80%E7%82%B9%E5%90%90%E6%A7%BD"><span class="toc-text">最后一点吐槽</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E8%80%85%E7%9A%84%E6%81%B6%E8%B6%A3%E5%91%B3"><span class="toc-text">作者的恶趣味</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E4%B8%80%E6%AE%B5%E6%98%AF%E4%BB%80%E4%B9%88%E4%B8%9C%E8%A5%BF%E5%95%8A"><span class="toc-text">最后一段是什么东西啊</span></a></li></ol></li></ol>
        </div>
    </aside>
    <a class="btn-toc button" id="btn-toc" tabindex="0">
        <svg viewBox="0 0 1024 1024" width="32" height="32" xmlns="http://www.w3.org/2000/svg">
            <path d="M128 256h64V192H128zM320 256h576V192H320zM128 544h64v-64H128zM320 544h576v-64H320zM128 832h64v-64H128zM320 832h576v-64H320z" fill="currentColor"></path>
        </svg>
    </a>
    <div class="toc-menus" id="toc-menus">
        <div class="toc-title">Article Directory</div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%93%88%E5%AF%86%E9%A1%BF%E5%8A%9B%E5%AD%A6%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A"><span class="toc-text">哈密顿力学必知必会</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E6%96%B9%E7%A8%8B"><span class="toc-text">正则方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E7%A9%BA%E9%97%B4"><span class="toc-text">相空间</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HNN"><span class="toc-text">HNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#baseline-nn"><span class="toc-text">baseline nn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HNN-1"><span class="toc-text">HNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-text">实例分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%90%E6%8C%AF%E5%AD%90"><span class="toc-text">谐振子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%8E%B0"><span class="toc-text">网络表现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90"><span class="toc-text">数量分析</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%AD%E7%A7%98HNN"><span class="toc-text">揭秘HNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%86%E5%A4%A9%E7%9A%84total-HNN-conserved-quantity"><span class="toc-text">逆天的total HNN-conserved quantity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%B4%A8%E6%98%AF%E6%8A%97%E6%89%B0%E5%8A%A8%E8%83%BD%E5%8A%9B%EF%BC%88or%E4%BF%9D%E8%BE%9B%E6%80%A7%EF%BC%9F%EF%BC%89"><span class="toc-text">本质是抗扰动能力（or保辛性？）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E4%B8%80%E7%82%B9%E5%90%90%E6%A7%BD"><span class="toc-text">最后一点吐槽</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E8%80%85%E7%9A%84%E6%81%B6%E8%B6%A3%E5%91%B3"><span class="toc-text">作者的恶趣味</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E4%B8%80%E6%AE%B5%E6%98%AF%E4%BB%80%E4%B9%88%E4%B8%9C%E8%A5%BF%E5%95%8A"><span class="toc-text">最后一段是什么东西啊</span></a></li></ol></li></ol>
    </div>


<article class="post post__with-toc content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <p>经典的感知机（MLP）、深度神经网络（DNN）从数据中学习规律。在面对来自物理问题中的数据时，假如直白地暴力训练，往往效果不会太好。</p>
<p>所以很多工作就考虑在神经网络中引入归纳偏置（inductive biases），可以理解为让网络包含对问题已有的一些先验的认知，比如deePMD用描述子（descriptor）在DNN中引入了平移、旋转、置换对称性，大幅提高了deep potential的训练效果。</p>
<p>这一类工作就叫PINN，physics informed neural network。</p>
<p>本文要介绍的<a target="_blank" rel="noopener" href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">哈密顿神经网络</a>（hamiltonian neural network, NIPS 2019）就是这样一篇工作。意图让通过调整架构让网络包含哈密顿量的某些性质，从而提高从数据中学习哈密顿量的效果。</p>
<p>体量不大，花了几个小时读了下，有点失望其实，感觉噱头大于实际。</p>
<h2 id="哈密顿力学必知必会"><a href="#哈密顿力学必知必会" class="headerlink" title="哈密顿力学必知必会"></a>哈密顿力学必知必会</h2><h3 id="正则方程"><a href="#正则方程" class="headerlink" title="正则方程"></a>正则方程</h3><p>哈密顿力学最早来自于经典力学，导出的哈密顿方程为</p>



$$
\dot{p_{\alpha}} = -\frac{\partial H}{\partial q_{\alpha}}\\
\dot{q_{\alpha}} = \frac{\partial H}{\partial p_{\alpha}}
$$




<p>$\alpha$为指标。</p>
<p>用动量$p$和位置（广义坐标）$q$足以描述一个系统的所有状态，故知道了一个系统的哈密顿量（$H$）就可以通过哈密顿方程得到这个系统演化的所有信息。哈密顿方程又叫哈密顿正则方程（hamiltonian canonical equation），canon一词有“正经”“典则”的含义，所以canonical在英文语境里有神圣的意味，可见这个方程的分量。</p>
<p>这个方程被广泛应用于经典力学、天体力学、量子力学等问题。</p>
<h3 id="相空间"><a href="#相空间" class="headerlink" title="相空间"></a>相空间</h3><p>我们熟知的欧几里得空间用$(x,y,z)$表示空间位置，相空间$(\vec{q},\vec{p} )$中的一点记录了组成某个系统的所有粒子的位置和动量($p,q$可以是很高维的)，包含了这个系统这个状态下的全部信息。</p>
<p>想知道一个系统随时间演化的过程需要知道</p>



$$
\frac{dp}{dt} = \dot p\\
\frac{dq}{dt} = \dot q
$$




<p>由正则方程知道，这正是哈密顿量在相空间的辛梯度（symplectic gradient）。<br>$$<br>S_{H} = (\frac{\partial H}{\partial p},-\frac{\partial H}{\partial q})<br>$$<br>有了初状态$(q_0, p_0)$后，可以很方便的得到系统演化过程中任一状态<br>$$<br>(q_1,p_1) = (q_0, p_0)+ \int_{t_0}^{t_1}S_H(q,p)dt<br>$$<br>可以用数值手段做到这件事，比如runge-kutta积分（RK4），追求保辛可以用verlet积分，这篇文章用的是scipy的RK4。</p>
<h2 id="HNN"><a href="#HNN" class="headerlink" title="HNN"></a>HNN</h2><p>可以进入正题了。</p>
<h3 id="baseline-nn"><a href="#baseline-nn" class="headerlink" title="baseline nn"></a>baseline nn</h3><p>明眼人都可以看出来，$S_H$其实才是真正想要的东西。所以非常straight forward的想法，找一个神经网络，输入$(q,p)$，输出$S_H$，来train这个东西。</p>
<p>非常不幸这种直来直去的东西只配作为baseline，被正主（HNN）吊打。不过我们后面会提到其实也没有被赢太多。</p>
<p><img src="/2022/05/08/post25/pic1.png" class="lazy" data-srcset="/2022/05/08/post25/pic1.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509201301488"></p>
<p>图来自论文，花体L是Loss，意思就是直接找和$S_H$的$L_2$ loss来train。</p>
<h3 id="HNN-1"><a href="#HNN-1" class="headerlink" title="HNN"></a>HNN</h3><p>那么这篇工作到底干了一件什么事呢，其实也非常的straight forward。</p>
<p>它不直接训练$S_H$，而是训练一个哈密顿量$H$。输入$(q,p)$，输出一个scalar，$H_{\theta}$。训练时候也不是直接训练这个scalar，而是训练它的辛梯度。</p>
<p>看看它的loss 。</p>
<p><img src="/2022/05/08/post25/pic2.png" class="lazy" data-srcset="/2022/05/08/post25/pic2.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509202335947"></p>
<p>就是说参数化以$(q,p)$为变量的哈密顿量，训练时也把$(\dot q, \dot p)$作为ground truth，用auto grad去算$H_{\theta}$的辛梯度，优化它们之间的$L_2$ loss。</p>
<p><strong>所以其实这两种方案都是在拟合相空间上的二维矢量场$S_H$。</strong></p>
<p>论文中展现的HNN的效果可谓逆天。</p>
<h3 id="实例分析"><a href="#实例分析" class="headerlink" title="实例分析"></a>实例分析</h3><p>原文中选用了多个场景作为算例，分别是谐振子、单摆、真实世界单摆（有摩擦，非保守）、二体运动，还有一个蛮重量级的从图片中学习单摆运动（Learning a Hamiltonian from Pixels）。</p>
<p>几个分析基本都是一个套路，仅取一例分析。</p>
<h4 id="谐振子"><a href="#谐振子" class="headerlink" title="谐振子"></a>谐振子</h4><p>对于最简单的谐振子可以直接写出它的hamiltonian，<br>$$<br>H = p^2 + q^2<br>$$</p>
<h5 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h5><p>可以知道对于任意一个初状态，之后的状态都落在圆轨迹上，把这叫做一个trajector，训练数据就利用$H$，在不同的trajector上采样，得到$(q, p, \dot p, \dot q ,t)$，事实上时间$t$在训练过程中是不必要的，是之后分析会用到。</p>
<p>$(q,p)$就是data，$(\dot q, \dot p)$就是label。这里有一个非常重的地方需要注意，生成训练数据的时候论文作者在data上加了噪声。</p>
<p><img src="/2022/05/08/post25/pic3.png" class="lazy" data-srcset="/2022/05/08/post25/pic3.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509204012836"></p>
<p>事实上在分析时，论文中都只有已在data上加早噪声的情况，我第一遍看的时候其实很奇怪，因为我读过deepSDF，这篇工作里加噪声是和未加噪声的情形做对比，来说明网络的鲁棒性。我直觉上不太能接受一篇工作里只分析加噪声的情形，就好比做数学题时肯定会先写上xx = 0的特殊情况，而不会一上来就找最一般的情况，而且只找一般情况。</p>
<p>这个地方我之后会说，我们先看下去。</p>
<p>网络训练起来收敛的非常快。</p>
<h5 id="网络表现"><a href="#网络表现" class="headerlink" title="网络表现"></a>网络表现</h5><p>在分析网络表现时，对比了groung truth（那个 H）、baseline、hnn。</p>
<p>都取$(q_0,p_0) = (1,0)$作为初始状态，然后利用<br>$$<br>(q_1,p_1) = (q_0, p_0)+ \int_{t_0}^{t_1}S_H(q,p)dt<br>$$<br>找相轨迹。其实就是用RK4，知道了$S_H$，解一个初值问题，得到了相轨迹上的散点，连起来就是欧拉折线、初值问题的数值解。</p>
<p><img src="/2022/05/08/post25/pic4.png" class="lazy" data-srcset="/2022/05/08/post25/pic4.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509205019561"></p>
<p>非常逆天！怎么会这样呢，用hnn得到的$S_H$形成的相轨迹完美贴合ground truth，而baseline nn得到$S_H$就很不好，得到的相轨迹有明显的耗散的迹象。</p>
<p>下面我们会探析这一点。</p>
<h5 id="数量分析"><a href="#数量分析" class="headerlink" title="数量分析"></a>数量分析</h5><p><img src="/2022/05/08/post25/pic5.png" class="lazy" data-srcset="/2022/05/08/post25/pic5.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509205350985"></p>
<p>这里的所谓 total HNN-conserved quantity，hnn那一条成了直线， 对于total energy，hnn那条线在ground truth周围波动，</p>
<p>论文里对此得出的结论是“HNN 确实得到了一个类似于保守系统里能量的东西”。</p>
<p>牛逼哄哄的（？）。</p>
<h2 id="揭秘HNN"><a href="#揭秘HNN" class="headerlink" title="揭秘HNN"></a>揭秘HNN</h2><h3 id="逆天的total-HNN-conserved-quantity"><a href="#逆天的total-HNN-conserved-quantity" class="headerlink" title="逆天的total HNN-conserved quantity"></a>逆天的total HNN-conserved quantity</h3><p>我第一次看到那张图的时候被吓了一跳：这效果也太好了！</p>
<p>其实这里面的道理非常平凡。</p>
<p>这里的分析是这样的，首先有三种方式得到各自的$S_H$，用RK4得到各自相轨迹上的散点$(q,p)$。然后把三条轨迹上的散点代入参数化的$H_{\theta}$也就是HNN做forward pass，得到scalar(array-like,exactly)，就是图中所示三条曲线。</p>
<p>通过这种方式的得到的hnn那条曲线是平直的几乎是一个必然——散点就是通过RK4用$H_{\theta}$算出的$S_H$得到的呀！就好比说，<strong>在同一条等高线上取到的点必然有一样的值。</strong></p>
<p>所以其实看energy更有参考价值，不过二者其实差不多一个意思。下面我们会说明，不管这能量曲线看起来有多牛逼，本质上其实就只说明了一件事，就是真实的相轨迹是个圈，hnn确实把这个圈找出来了。</p>
<p>而energy会出现一个周期性的波动，主要是因为RK4比较好，它的折现也是绕这个闭环，不会跑出去，假如换一个积分算法就未必了。</p>
<h3 id="本质是抗扰动能力（or保辛性？）"><a href="#本质是抗扰动能力（or保辛性？）" class="headerlink" title="本质是抗扰动能力（or保辛性？）"></a>本质是抗扰动能力（or保辛性？）</h3><p>前面我们提到，本文只有噪声情况下的分析。我想说，其实<strong>HNN的feature就是抗噪声</strong>。</p>
<p>我找到了他们的<a target="_blank" rel="noopener" href="https://github.com/greydanus/hamiltonian-nn">code</a>，不得不说他们的code写的非常好，我学到许多。</p>
<p>在生成数据时把加噪声那两行给注释掉，</p>
<p><img src="/2022/05/08/post25/pic3.png" class="lazy" data-srcset="/2022/05/08/post25/pic3.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509204012836"></p>
<p>去看训练效果。</p>
<p><img src="/2022/05/08/post25/pic6.png" class="lazy" data-srcset="/2022/05/08/post25/pic6.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509211406760"></p>
<p>效果一样了！但其实如果多跑几步，会发现baseline nn还是会出现耗散，但耗散非常小，远低于加噪声情况下的效果。</p>
<p>到这里我们其实可以得出一个结论，只有在噪声情况下，hnn才有比baseline好的多的情况。</p>
<p>道理其实很简单。</p>
<p>baseline没有任何约束，你给它什么数据他就从数据里学到什么。</p>
<p>上面我们说，两种方案都是在拟合一个场。你给了它一个有噪声的场，它也照样把噪声给学进来，自然不符合理想的保守场的情况。但如果数据没噪声，它是有能力学到一个比较好的场的！</p>
<p>而对于hnn，它的场是由一个scalar做新梯度得到的，所以它在学的时候其实就没那么自由，相当于是受到了约束，<strong>只能学到满足某种全微分关系的场</strong>；即便训练数据加了噪声，它也可以学到保守的场，噪声对它的影响不会太大。</p>
<p>有点像保辛性了。</p>
<p>它一定可以从数据里学到首尾封闭的相轨迹，RK4一定可以把这个轨迹画出来。</p>
<p>那么这样看来，hnn一点也不神奇，作者不做无噪声和加噪声的目的也昭然若揭——一对比就显的hnn太平凡了！</p>
<p>其实也没做什么事嘛，但确实挺好用的，有点隐式正则化那味道。</p>
<h2 id="最后一点吐槽"><a href="#最后一点吐槽" class="headerlink" title="最后一点吐槽"></a>最后一点吐槽</h2><p>到这里其实这篇工作就讲完了，比较有意思的其实是Learning a Hamiltonian from Pixels的应用，把一组单摆摆动中的图片用autoencoder得到latent space，然后把这个latent space作为相空间。在这里面得到相轨迹后再decoder得到图片，非常好玩。可以在他们的<a href="(https://github.com/greydanus/hamiltonian-nn)">repo</a>里找到。</p>
<h3 id="作者的恶趣味"><a href="#作者的恶趣味" class="headerlink" title="作者的恶趣味"></a>作者的恶趣味</h3><p><img src="/2022/05/08/post25/pic7.png" class="lazy" data-srcset="/2022/05/08/post25/pic7.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509155644984"></p>
<p>salt of the earth MLP😂</p>
<p><img src="/2022/05/08/post25/pic8.png" class="lazy" data-srcset="/2022/05/08/post25/pic8.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509213130340"></p>
<p>hamilton是吧（</p>
<h3 id="最后一段是什么东西啊"><a href="#最后一段是什么东西啊" class="headerlink" title="最后一段是什么东西啊"></a>最后一段是什么东西啊</h3><p><img src="/2022/05/08/post25/pic9.png" class="lazy" data-srcset="/2022/05/08/post25/pic9.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABAQMAAAAl21bKAAAABlBMVEXMzMyWlpYU2uzLAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAACklEQVQImWNgAAAAAgAB9HFkpgAAAABJRU5ErkJggg==" alt="image-20220509213235740"></p>
<p>我懂你可逆性的意思。</p>
<p>是说神经网络在计算反向传播时，需要保存每一个中间值，假如神经网络规模一大，内存就炸了。很多工作就在考虑设计架构，可否实现每一层的反向传播都可以从上一层重建，而不依赖于全局的中间值。</p>
<p>能否精确重建反向传播，这个叫可逆性。</p>
<p>刘维尔定理，非耗散情况下两个点间存在双射，也是可逆性。</p>
<p>但这两个怎么看都不是同一个可逆性啊？？？？？</p>
<p>酱紫水文章？欺负审稿人不懂刘维尔定理？？？</p>
<p>也许是我没搞懂前一个可逆性，二者间真的存在深刻的联系也不一定。之后有机会再看看相关文章。</p>
<p>这周可能会再看几篇PINN的，得学学GNN了。</p>

    </div>
     
    <div class="post-footer__meta"><p>updated at 2022-05-09</p></div> 
    <div class="post-entry__tags"><a href="/tags/machine-learning/" class="post-tags__link button"># machine learning</a><a href="/tags/%E7%BB%8F%E5%85%B8%E5%8A%9B%E5%AD%A6/" class="post-tags__link button"># 经典力学</a></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
        </div>
        <div class="nav__next">
            
                <a href="/2022/04/17/post24/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            Next Post
                        </div>
                        <div class="nav__title">
                            十六日谈-02-dead in the water
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
     
    <a href="#" class="button" id="b2t" aria-label="Back to Top" title="Back to Top">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="32" height="32">
            <path d="M233.376 722.752L278.624 768 512 534.624 745.376 768l45.248-45.248L512 444.128zM192 352h640V288H192z" fill="currentColor"></path>
        </svg>
    </a>

    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2022 <a href="/">狮子大张嘴&#39;s blog</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
        
    <script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
    <script>
        window.lazyLoadOptions = {
            elements_selector: ".lazy",
            threshold: 0
        };
    </script>
 

 

 

 

 



 



 


    
 

 


    <script>
        if (typeof MathJax === 'undefined') {
            window.MathJax = {
                loader: {
                    source: {
                        '[tex]/amsCd': '[tex]/amscd',
                        '[tex]/AMScd': '[tex]/amscd'
                    }
                },
                tex: {
                    inlineMath: {'[+]': [['$', '$']]},
                    tags: 'ams'
                },
                options: {
                    renderActions: {
                        findScript: [10, doc => {
                            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                                const display = !!node.type.match(/; *mode=display/);
                                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                                const text = document.createTextNode('');
                                node.parentNode.replaceChild(text, node);
                                math.start = {node: text, delim: '', n: 0};
                                math.end = {node: text, delim: '', n: 0};
                                doc.math.push(math);
                            });
                        }, '', false],
                        insertedScript: [200, () => {
                            document.querySelectorAll('mjx-container').forEach(node => {
                                let target = node.parentNode;
                                if (target.nodeName.toLowerCase() === 'li') {
                                    target.parentNode.classList.add('has-jax');
                                }
                            });
                        }, '', false]
                    }
                }
            };
            (function () {
                var script = document.createElement('script');
                script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
                script.defer = true;
                document.head.appendChild(script);
            })();
        } else {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
        }
    </script>
 

 

 

 




    </body>
</html>
