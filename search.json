[{"title":"PINN-01-哈密顿神经网络（HNN）","date":"2022-05-07T17:31:23.000Z","url":"/2022/05/08/post25/","tags":[["machine learning","/tags/machine-learning/"],["经典力学","/tags/%E7%BB%8F%E5%85%B8%E5%8A%9B%E5%AD%A6/"]],"categories":[["每一步路都算数","/categories/%E6%AF%8F%E4%B8%80%E6%AD%A5%E8%B7%AF%E9%83%BD%E7%AE%97%E6%95%B0/"]],"content":"经典的感知机（MLP）、深度神经网络（DNN）从数据中学习规律。在面对来自物理问题中的数据时，假如直白地暴力训练，往往效果不会太好。 所以很多工作就考虑在神经网络中引入归纳偏置（inductive biases），可以理解为让网络包含对问题已有的一些先验的认知，比如deePMD用描述子（descriptor）在DNN中引入了平移、旋转、置换对称性，大幅提高了deep potential的训练效果。 这一类工作就叫PINN，physics informed neural network。 本文要介绍的哈密顿神经网络（hamiltonian neural network, NIPS 2019）就是这样一篇工作。意图让通过调整架构让网络包含哈密顿量的某些性质，从而提高从数据中学习哈密顿量的效果。 体量不大，花了几个小时读了下，有点失望其实，感觉噱头大于实际。 哈密顿力学必知必会正则方程哈密顿力学最早来自于经典力学，导出的哈密顿方程为$$\\dot{p_{\\alpha}} = -\\frac{\\partial H}{\\partial q_{\\alpha}}\\\\dot{q_{\\alpha}} = \\frac{\\partial H}{\\partial p_{\\alpha}}$$$\\alpha$为指标。 用动量$p$和位置（广义坐标）$q$足以描述一个系统的所有状态，故知道了一个系统的哈密顿量（$H$）就可以通过哈密顿方程得到这个系统演化的所有信息。哈密顿方程又叫哈密顿正则方程（hamiltonian canonical equation），canon一词有“正经”“典则”的含义，所以canonical在英文语境里有神圣的意味，可见这个方程的分量。 这个方程被广泛应用于经典力学、天体力学、量子力学等问题。 相空间我们熟知的欧几里得空间用$(x,y,z)$表示空间位置，相空间$(\\vec{q},\\vec{p} )$中的一点记录了组成某个系统的所有粒子的位置和动量($p,q$可以是很高维的)，包含了这个系统这个状态下的全部信息。 想知道一个系统随时间演化的过程需要知道$$\\frac{dp}{dt} = \\dot p\\\\frac{dq}{dt} = \\dot q$$由正则方程知道，这正是哈密顿量在相空间的辛梯度（symplectic gradient）。$$S_{H} = (\\frac{\\partial H}{\\partial p},-\\frac{\\partial H}{\\partial q})$$有了初状态$(q_0, p_0)$后，可以很方便的得到系统演化过程中任一状态$$(q_1,p_1) = (q_0, p_0)+ \\int_{t_0}^{t_1}S_H(q,p)dt$$可以用数值手段做到这件事，比如runge-kutta积分（RK4），追求保辛可以用verlet积分，这篇文章用的是scipy的RK4。 HNN可以进入正题了。 baseline nn明眼人都可以看出来，$S_H$其实才是真正想要的东西。所以非常straight forward的想法，找一个神经网络，输入$(q,p)$，输出$S_H$，来train这个东西。 非常不幸这种直来直去的东西只配作为baseline，被正主（HNN）吊打。不过我们后面会提到其实也没有被赢太多。 图来自论文，花体L是Loss，意思就是直接找和$S_H$的$L_2$ loss来train。 HNN那么这篇工作到底干了一件什么事呢，其实也非常的straight forward。 它不直接训练$S_H$，而是训练一个哈密顿量$H$。输入$(q,p)$，输出一个scalar，$H_{\\theta}$。训练时候也不是直接训练这个scalar，而是训练它的辛梯度。 看看它的loss 。 就是说参数化以$(q,p)$为变量的哈密顿量，训练时也把$(\\dot q, \\dot p)$作为ground truth，用auto grad去算$H_{\\theta}$的辛梯度，优化它们之间的$L_2$ loss。 所以其实这两种方案都是在拟合相空间上的二维矢量场$S_H$。 论文中展现的HNN的效果可谓逆天。 实例分析原文中选用了多个场景作为算例，分别是谐振子、单摆、真实世界单摆（有摩擦，非保守）、二体运动，还有一个蛮重量级的从图片中学习单摆运动（Learning a Hamiltonian from Pixels）。 几个分析基本都是一个套路，仅取一例分析。 谐振子对于最简单的谐振子可以直接写出它的hamiltonian，$$H = p^2 + q^2$$ 训练数据可以知道对于任意一个初状态，之后的状态都落在圆轨迹上，把这叫做一个trajector，训练数据就利用$H$，在不同的trajector上采样，得到$(q, p, \\dot p, \\dot q ,t)$，事实上时间$t$在训练过程中是不必要的，是之后分析会用到。 $(q,p)$就是data，$(\\dot q, \\dot p)$就是label。这里有一个非常重的地方需要注意，生成训练数据的时候论文作者在data上加了噪声。 事实上在分析时，论文中都只有已在data上加早噪声的情况，我第一遍看的时候其实很奇怪，因为我读过deepSDF，这篇工作里加噪声是和未加噪声的情形做对比，来说明网络的鲁棒性。我直觉上不太能接受一篇工作里只分析加噪声的情形，就好比做数学题时肯定会先写上xx = 0的特殊情况，而不会一上来就找最一般的情况，而且只找一般情况。 这个地方我之后会说，我们先看下去。 网络训练起来收敛的非常快。 网络表现在分析网络表现时，对比了groung truth（那个 H）、baseline、hnn。 都取$(q_0,p_0) = (1,0)$作为初始状态，然后利用$$(q_1,p_1) = (q_0, p_0)+ \\int_{t_0}^{t_1}S_H(q,p)dt$$算相轨迹，其实就是用RK4，知道了$S_H$，解一个初值问题，得到了相轨迹上的散点，连起来就是欧拉折线、初值问题的数值解。 非常逆天！怎么会这样呢，用hnn得到的$S_H$形成的相轨迹完美贴合ground truth，而baseline nn得到$S_H$就很不好，得到的相轨迹有明显的耗散的迹象。 下面我们会探析这一点。 数量分析 这里的所谓 total HNN-conserved quantity，hnn那一条成了直线， 对于total energy，hnn那条线在ground truth周围波动， 论文里对此得出的结论是“HNN 确实得到了一个类似于保守系统里能量的东西”。 牛逼哄哄的！ 揭秘HNN逆天的total HNN-conserved quantity我第一次看到那张图的时候被吓了一跳：这效果也太好了！ 其实这里面的道理非常平凡。 这里的分析是这样的，首先有三种方式得到各自的$S_H$，用RK4得到各自相轨迹上的散点$(q,p)$。然后把三条轨迹上的散点代入参数化的$H_{\\theta}$也就是HNN做forward pass，得到scalar(array-like,exactly)，就是图中所示三条曲线。 通过这种方式的得到的hnn那条曲线是平直的几乎是一个必然——散点就是通过RK4用$H_{\\theta}$算出的$S_H$得到的呀！就好比说，在同一条等高线上取到的点必然有一样的值。 所以其实看energy更有参考价值，不过二者其实差不多一个意思。 本质是抗扰动能力（or保辛性？）前面我们提到，本文只有噪声情况下的分析。我想说，其实HNN的feature就是抗噪声。 我找到了他们的code，不得不说他们的code写的非常好，我学到许多。 在生成数据时把加噪声那两行给注释掉， 去看训练效果。 效果一样了！但其实如果多跑几步，会发现baseline nn还是会出现耗散，但耗散非常小，远低于加噪声情况下的效果。 到这里我们其实可以得出一个结论，只有在噪声情况下，hnn才有比baseline好的多的情况。 道理其实很简单。 baseline没有任何约束，你给它什么数据他就从数据里学到什么。 上面我们说，两种方案都是在拟合一个场。你给了它一个有噪声的场，它也照样把噪声给学进来，自然不符合理想的保守场的情况。但如果数据没噪声，它是有能力学到一个比较好的场的！ 而对于hnn，它的场是由一个scalar做新梯度得到的，所以它在学的时候其实就没那么自由，相当于是受到了约束，只能学到满足某种全微分关系的场；即便训练数据加了噪声，它也可以学到保守的场，噪声对它的影响不会太大。 有点像保辛性了。 它一定可以从数据里学到首尾封闭的相轨迹，RK4一定可以把这个轨迹画出来。 那么这样看来，hnn一点也不神奇，作者不做无噪声和加噪声的目的也昭然若揭——一对比就显的hnn太平凡了！ 其实也没做什么事嘛，但确实挺好用的，有点隐式正则化那味道。 最后一点吐槽到这里其实这篇工作就讲完了，比较有意思的其实是Learning a Hamiltonian from Pixels的应用，把一组单摆摆动中的图片用autoencoder得到latent space，然后把这个latent space作为相空间。在这里面得到相轨迹后再decoder得到图片，非常好玩。可以在他们的repo里找到。 作者的恶趣味 salt of the earth MLP😂 hamilton是吧（ 最后一段是什么东西啊 我懂你可逆性的意思。 是说神经网络在计算反向传播时，需要保存每一个中间值，假如神经网络规模一大，内存就炸了。很多工作就在考虑设计架构，可否实现每一层的反向传播都可以从上一层重建，而不依赖于全局的中间值。 能否精确重建反向传播，这个叫可逆性。 刘维尔定理，非耗散情况下两个点间存在双射，也是可逆性。 但这两个怎么看都不是同一个可逆性啊？？？？？ 酱紫水文章？欺负审稿人不懂刘维尔定理？？？ 也许是我没搞懂前一个可逆性，二者间真的存在深刻的联系也不一定。之后有机会再看看相关文章。 这周可能会再看几篇PINN的，得学学GNN了。"},{"title":"十六日谈-02-dead in the water","date":"2022-04-16T16:46:01.000Z","url":"/2022/04/17/post24/","tags":[["重大说明","/tags/%E9%87%8D%E5%A4%A7%E8%AF%B4%E6%98%8E/"],["Stank Tones","/tags/Stank-Tones/"]],"categories":[["这个男人来自地球","/categories/%E8%BF%99%E4%B8%AA%E7%94%B7%E4%BA%BA%E6%9D%A5%E8%87%AA%E5%9C%B0%E7%90%83/"]],"content":"上一篇十六日谈是在15号发的，这篇十六日谈是在17号发的。平均下来那也都是十六日发的，非常合理。（x） 总归是迟到啦，从三月半到四月半，发生了太多事情。 首先是发现自己身上存在某种摆烂的周期律，高效率运转一段时间后，可能是一周，效率就会突然间无以为继，陷入某种泥潭。其实我感觉这可能和我周期性失眠有关系，只要有一天我只睡了三个小时，之后的好几天都烂了，这样的事情一周至少发生一次。 褪黑素还是不太想吃，据说吃了之后内源性褪黑素分泌就会更疲软无力，所以还是很希望可以通过自己的努力把作息调整回来。下个月绝不摆烂！ 然后是过了五四青春诗会的面试。离自己在本科完成一次上台表演的目标又近了一步，希望之后文体部不要摆烂（x）。 慢慢的慢慢的就忙起来了，之前还有闲心一晚上写一篇五千字长文，但早就没有那种条件了，这个学期最闲的时光已经过去了，之后的目标是活下去。 这个月也没读啥书，没啥好说的，就。 关于信仰总感觉信仰是一种神奇海螺。 是一个给定输入，返回输出的系统，或许是pure function，假如一个人的信仰不以环境为转移。 在遇到一个终极问题，或者依赖有限经验无法得到解答的问题时，我们往往诉诸信仰。从别人那拿的，或者自己造的，我们有了信仰，于是那些笼罩着千百年未揭开面纱的问题，也有了答案。 我们把思考交给这样一个机械化系统，放弃了自己思索的权利，“为什么不问问神奇海螺呢？”，有点偷懒呢是不是。 关于力学抽象程度高的学科进入抽象程度低的学科往往可以爆杀，在抽象程度低的学科领域里做出很多有价值的成果，这一点已经被反复证明了。 但这种结合是不长久、不稳定的，之后要么下沉，专精具体问题；要么上升，发展抽象理论。 但作为理论科学的力学看起来是一个另类。力学应该是先贤最早将“自然哲学”那一套思想应用到生活现象、工程问题取得大量成果的范例，极尽光彩。但现在力学似乎早已不现辉煌。 道理很简单，随着各门学科的深入发展，当今的工程问题早已脱离了土木工程的范畴，数学的发展也超出一般人的认知，力学统治的在当时看来的无限未来，现在只是区区一亩三分地。 往上走，力学的数学工具就是古典场论，能有多少空间？往下走，传统工程早已不是当今的热点。落寞实在是太正常了，更不用说，即便是传统工程领域，大部分痛点也不是力学的手段可以解决的，所以生物力学可能是好东西？ 于是一帮子人，把这一亩三分地玩出了花。现在看到当年的弹性力学教程，感慨一声先辈解Pde功力深厚之余，也会吐槽： 这年头谁用手算啊。"},{"title":"并行计算-04-gcc&makefile","date":"2022-04-09T17:52:04.000Z","url":"/2022/04/10/post23/","tags":[["并行计算","/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"]],"categories":[["去码头整点薯条","/categories/%E5%8E%BB%E7%A0%81%E5%A4%B4%E6%95%B4%E7%82%B9%E8%96%AF%E6%9D%A1/"]],"content":"GCC基本语法 根据源代码编译文件，中间是选项。 最简单的就是直接编译，然后运行 这里的a.out应该就是可执行的输出文件，不是.exe啊，这里头的区别还得在看看。 ./是啥一般性的运行命令吗，以及输出文件的名称a是默认的吗？都不知道欸。 附注：不是什么东西都可以往.h里写 选项-Dmacro[=defn]可以定义宏，人为塞进来一个东西控制编译（相当于输入参数，源码中会有接口） 如使用单精度 USE_SINGLE是在程序里有的接口。 -I头文件路径添加头文件搜索路径。假如自己写了个头文件在include里， 会报错，因为编译器不到头文件在哪。 需要加 ../表示上一层目录，记住-I和后面路径间没有空格。 即 编译器版本然后会继续报错。老版本编译器不支持新特性。 比如 这个闭包是个好东西，但老版本不支持。 需要在后面加 链接到库-l同一个名字的函数可能在不同库里有不同的实现，故需要指定。 用 相当于链接到libmath.so，这就是lm的含义。l代表lib，所有库的开头格式。 意思是链接到数学库。需注意有书写顺序。 附注：头文件只是告诉了函数的声明，没有说函数的实现，具体实现都在`.so里 到这里就可以编译出a.out了。 警告所有 打开然后警告所有可能的错误，这种时候编译器还是很聪明的。 高级选项 foption打开某种特性比如-fopenmp -o0,-o1：打开优化选项，让程序更快。 编译过程把某个文件变成共享库如果没有main，没法编译为可执行文件。 加上第一个就行，第二个也一定也加，不然容易出问题，第三个是共享库的名称格式。 连上自己的库 这是连接库的默认格式。 库搜索路径默认不会在main.c（待编译文件）同层级位置下搜索库，要加 -L. 是声明在当前目录搜索。 但这样还是会报错。 但谁说我编译时候用到的库就是我运行时候用到的库？编译时运行时库查找路径不同。 绕起来了（x）. 运行时动态库查找路径 告诉程序运行时用到的库就在当前可执行文件所在的目录。 查看可执行文件的信息 显示可执行文件的运行依赖库等等。根据输入的信息就把程序写死了。 Makefile在复杂的依赖关系下，一个个来太麻烦了，here comes Makefile! 写了这个脚本后，用Make程序，输入Make就行了。 基本规则 targets:产物 prerequisite:产物依赖 recipe:生成方法，前面必须有。其实就是shell 命令。 其实就是把bash里该写的命令写一起了。 从终产物到依赖产物，一层层写下去。 注意假如产物依赖在其他文件层级下，那要把路径写上。本质上产物依赖写上去的都是路径，同层级下直接写文件名。 注意recipe更新了产物不会自动更新，需要把中间产物删掉。 GDB不写啦，记录下上课提到的bug: a.size()返回unsigned long，一直减一会出问题，达到边界后就炸了。"},{"title":"并行计算-03-集群上提交作业","date":"2022-04-09T13:42:09.000Z","url":"/2022/04/09/post22/","tags":[["并行计算","/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"]],"categories":[["去码头整点薯条","/categories/%E5%8E%BB%E7%A0%81%E5%A4%B4%E6%95%B4%E7%82%B9%E8%96%AF%E6%9D%A1/"]],"content":"slurm脚本 这部分叙述了作业的属性。 然后就是命令行里咋写，接下来咋写。 …诸如此类。但似乎slurm调度资源主要还是根据task的数量？ 写完脚本用 就可以提交运行了。 交互式运行用salloc申请分配资源，然后就可以像用命令行那样用了。不过用完需要及时退出，不然会一直烧机时。 但为什么说要用srun来运行程序呢？我在未名一号上就没用过。 其他命令squeue：查看队列情况。 tail:查看job输出文件"},{"title":"并行计算-02-linux terminal常用命令","date":"2022-04-09T12:36:51.000Z","url":"/2022/04/09/post21/","tags":[["并行计算","/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"]],"categories":[["去码头整点薯条","/categories/%E5%8E%BB%E7%A0%81%E5%A4%B4%E6%95%B4%E7%82%B9%E8%96%AF%E6%9D%A1/"]],"content":" 命令的结构如上。 目录相关pwd查看当前目录 cdcd [目录],进入目录。 cd ..返回上级目录 cd ~~代表所在集群里自己账户的Home位置， cd -进入上一个工作目录。 ls列出目录下所有文件（夹）。 按层级递归列出目录下所有文件。 文件相关权限u``w``x分别代表可读，可写，可执行。 命令可以用来修改文件权限。 mkdir mkdir [目录],直接创建 mkdir -p [目录]，-p为选项，后面跟着一串目录，表示按此路径创建，可以逐级创建。 cp复制文件。 复制文件夹及内部所有到指定目录。 mv和cp类似。 rm其实类似，如果想删文件夹option就是-r。 查看文件 cat [文件名]：一口气把文件里所有内容显示在屏幕上，其实就是全部打印出来。 more&amp;less。功能比cat更强大，more可以翻页，一行行显示。 vim tabe include\\axphy.h：可以开新标签页打开文件。 压缩包 帮助文档来自并行计算课讲义。“不会用就man一下” 组合键ctrl + z：挂起当前程序到后台。tab: 自动补全。"},{"title":"机器学习-04-some mathematical perspectives(2)","date":"2022-04-05T14:46:15.000Z","url":"/2022/04/05/post20/","tags":[["machine learning","/tags/machine-learning/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"从误差分解的角度，分析了维度灾难（CoD）的成因；在此基础上，用我看不懂的定理分析了机器学习中的误差。 误差分解机器学习得到的模型我们记为$\\hat f$，真实模型为$f$，$f-\\hat f$可以分解为 $$f-\\hat f = f- f_m+f_m -f_n +f_n-f$$ $f-f_m$在某个函数空间(hypotheis space)下对$f$产生的最佳逼近的误差，approximation error，可以理解为模型误差 $f_m -f_n$，$f_n$为机器学习中利用有限数据，即优化empircal risk得到的函数，和$f_m$存在误差，estimation error，实际上是generalization gap $f_n - \\hat f$是优化loss时的误差，loss一般没法GD（gradient descent）到0，这又是一波误差 通过分别分析三个误差，可以理解不同因素导致的机器学习中误差。 维数灾难（CoD）我们先来看第一个误差，分析这一部分可以得到对CoD的理解。 传统方法中的模型误差以数值积分为例，Grid-based quadrature rules： $$I(g)-I_n(g) \\sim \\frac{C}{m^{\\frac{\\alpha}{d} }}$$ 大意就是，用传统的画格子积分，如果$\\alpha=1$,误差中$m^{-\\frac{\\alpha}{d}}=0.1$的话，会有$m=10^d$，就是说离散程度$m$是维度$d$的指数项。 意思就是说，在高维情况下用这种格子离散化的积分，想要达到理想的误差，会要求很高的离散程度，这对计算的要求是很恐怖的。 这就是维数灾难，计算代价随维数指数级增大。 事实上，对于其他传统方法，比如样条插值小波变换等，在一组basis function下的拟合，都存在这种问题。 Monte Carlo积分蒙特卡洛积分是用样本平均值来替代积分值。 在$x \\sim uniform(a,b)$时，取 $$g(x_n) =\\frac {\\sum_{i}^{n}g(x_i)}{n}$$ 可以证明，样本平均值的期望满足 $$(b-a)E[g(x_n)] = \\int_a^b g(x)dx =I$$ 可以证明这个方法的误差 $$E[(I(g)-I_m(g))^2] = \\frac{var(g)}{m}$$ 破除CoD这是非常exciting的！只要样本个数取的足够多，无论维数几何，误差都可以以$m^{-1}$的速度接近于0。 也正是因为这种特性，蒙特卡洛积分被广泛应用于物理中的高维积分。 这样看，蒙特卡洛积分好像是一种破除CoD的方法。但事实上在高维上$var(g)$是可以很大的，会要求取更多的样本数。 新视角下的generaliaztion gap在前一篇博文机器学习-03-some mathematical perspectives(1)中提到，机器学习优化的是empirical loss，但真正关心的是全局上的积分population loss，二者的差距就叫generalization gap。 empirical loss是样本（training data）上的误差，而样本是随机取得的——那么是不是可以把empirical loss 看成是对population loss的一个蒙特卡洛积分？这样子二者间的误差也是以$m^{-1}$的速度接近于0的？ 想法很exciting，但很遗憾不是的。因为在机器学习中$g(x_n)$是和样本$x_n$有关的，也就是说不同的training data会出现不同的参数，这样子误差的估计就不会有非常整齐的形式，能有整齐的$\\frac{var(g)}{m}$是因为变量之间无关，统统消掉了！ 可以联想到龙格库塔（runge-kutta）现象，不同采样点可以拟合出各式各样千奇百怪的曲线，与真实曲线相差甚远。 Rademacher Complexity放弃了，这里看不懂，以后有机会补上。 好像就是说，最后机器学习模型的generalization gap还是可以被控制住，控制在 $$\\frac{\\gamma_1(f^*)}{m}+\\frac{\\gamma_2(f^*)}{n^\\frac{1}{2}}$$ 对于resnet，二层神经网络都是被这个Monte Carlo bounds控制，没有CoD。 另外一件有意思的事，Hypothesis Space为全体连续函数时，Rademacher复杂度是$O(1)$过大；为李普希兹1函数时，就是小于$0.1$，此时的generalization gap $\\sim n^{-\\frac{1}{d}}$。"},{"title":"梦蝶-射雕三部曲&《天龙八部》-满怀愧疚的活下去吧","date":"2022-03-31T15:10:07.000Z","url":"/2022/03/31/post19/","tags":[["读书笔记","/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"],["Stank Tones","/tags/Stank-Tones/"]],"categories":[["这个男人来自地球","/categories/%E8%BF%99%E4%B8%AA%E7%94%B7%E4%BA%BA%E6%9D%A5%E8%87%AA%E5%9C%B0%E7%90%83/"]],"content":"又开了个系列！这个系列是读书笔记或者观影笔记，记录的是自己的感受和自认为还算有趣的想法。冠以“梦蝶”之名，因为就目前我认为读书观影的意义是经历别样的人生，获取在daily route中无缘体会的经验；困穷的书生短暂在梦中化蝶，翩翩起舞。这一篇是我阅读金庸小说，射雕三部曲和天龙八部的读书笔记。 我开始看金庸是上学期期中，考数理方法下期中，走楼梯时候把手机屏幕磕坏了。哇屏幕上好大一条亮带，亮瞎眼了欸，不用手电筒了欸，屏幕怼那一照比啥都亮，哈哈。修好手机前，都没法看了。那就找点别的看，所以我从图书馆借了金庸小说！《射雕英雄传》《神雕侠侣》《倚天屠龙记》《天龙八部》是比较能体现金庸代表风格的作品，而且也在同一个世界观下，本篇博文就一并写了。 本博文的主要内容和“满怀愧疚的活下去”这个标题其实没啥关系…那是写在最后的。 abstract(🐶)以金庸三部曲为代表，本文尝试对分析金庸小说中的人物和矛盾提供一种框架，对加深理解小说中人物和情节发展有潜在的帮助。 “宗派”与“豪雄”：人物的两个标签在研究小说的理论中，我这个分析应该是把人物视作情节的附属，属于俄罗斯形式主义学派？ “阵营”标签我是从初中开始看网络小说，阅读了大量了玄幻小说。主流玄幻小说一个非常鲜明的特点就是，围绕门派，宗室展开情节。主角往往一开始就在一个门派或者家族里面，往往又是被人看不起的货色，为了在门派中崭露头角主角拼命修行或者是突然一天开了金手指，于是大杀四方，“三十年河东三十年河西”，把以前看不起他的人统统踩在脚下，赢得美人归。终于在门派里获得了较高的地位，好像可以过上红袖添香的好日子，突然天降噩耗，一个更强的门派降临，把主角好不容易过上的好日子打个七零八落，于是挥泪开新地图，换个地方练级。 诚然，这些小说的主线都是主角不断修行提高自己的等级，由此展开情节；但不管是主角的耻辱、获取资源、崭露头角、遭遇的灾难，统统都是在特定的门派、宗室之内或者之间发生的，比如主角往往通过门派大比崭露头角又结实新的仇家；到故事最后往往又是主角登临顶点，为了对抗某种“终极邪恶”大家都团结起来。 大而化之，这些小说矛盾的出现完完全全是依靠阵营的冲突、转换；而情节的进一步展开完完全全是依靠阵营的扩大——也就是俗称的换地图，旧地图上主角一统所有阵营，故事说不下去了咋办，告诉主角远方还有更多的敌人——我们这不过是小地方呐！格局打开，主角从过去的小阵营钻进了大阵营，继续吭哧吭哧升级了，大阵营之上还有更大的阵营！故事就这么铺开了，字数就这么水到几千万字了。 一切，都是人物的“阵营”这个标签在轮转，从而产出故事。 金庸小说中的“宗派”与“豪雄”金庸小说和上面介绍的玄幻小说可以说是大相径庭。射雕三部曲&amp;天龙八部同样有“阵营”标签的存在，为了适应武侠小说的风格，我把这叫做“宗派”标签，比如小说的门派、家族、国家，统统用这个“宗派标签”概括，不过我们把家国之仇当作某种本底背景出现，之后单独拿出来考虑。 “豪雄”标签比较好说，指的是当人物做出了那些你看了不得不喊一声“好汉子”的行为的时候，就是“豪雄”标签在发挥作用。金庸小说里刻画了好多豪雄，我最喜欢的一个就是萧峰。 事实上一个人物不可能单单有“宗派”和“豪雄”标签，两个标签往往是互耦的；用一个更加清晰的定义，“宗派”标签和“豪雄”标签的区别在于，人物的行动是更多出于结构性的因素还是人物个性的因素。换句话说，当一个人物做了在这个阵营背景下这个阵营位置上的人都该做的事情，那就是“宗派”标签在起作用，刻画了阵营间的结构化矛盾；当一个人物做了这个性格的人可能做的事情，那就是“豪雄”标签在起作用，塑造并强化了人物形象。举个例子，张无忌和周芷若是一对为情所困的江湖男女，但他们在少林屠狮大会上相见时，就是明教教主和峨嵋派掌门人。 到这里我们可以先下个结论，在射雕三部曲&amp;《天龙八部》中，“宗派”标签起的作用是在慢慢强化的，但总体看来，“豪雄”标签强于“宗派”标签，即金庸更着重在江湖的恩怨情仇中刻画人物形象。 《射雕英雄传》里没有宗派到这里大家可能会好奇，这本书里哪里没有宗派了，全真教不是宗派吗，丐帮不是宗派吗，且听我道来。 以《射雕英雄传》的全真教为例，整本书里至始至终，全真教的人物都只有“全真七子”出现，也有他们的徒弟出现，但他们的徒弟身上的标签就只是“全真七子的徒弟”而非“全真教门人”。这是一个很有意思的现象，他们所作的事，比如马钰教导郭靖，七子共斗西毒，都和全真教这个宗派没关系，毫无关系，都是他们出于个人意志的行动！ 包括丐帮，丐帮帮主洪七公四处云游，结交各路好汉、吃吃喝喝，除了传位给黄蓉，他干的没有一件事是和“丐帮帮主”这个身份有关系的。包括黄蓉接了丐帮帮主之外，她也只是多了根棒子和几门武功，这个人物完全没有和丐帮这个宗派绑定在一起。还可以做出很多类似的分析。 除了作为本底背景的宋国和蒙古国的国家对立，一切的情节从来以“豪雄”为中心而展开，比如说桃花岛三代恩恩怨怨，武林五大高手比拼搏斗，江南七怪情比金坚……你从这本书里可以看到一个个鲜活的人物，但是看不到一个热热闹闹的武侠世界，这些个人物确实有血有肉，但少了众声喧天，显的有些空旷。 “宗派”标签的强化在后面几本书中“宗派”标签是慢慢强化的，可见金庸在写作中水平有所提升（x），笔下的江湖世界越来越细节翔实了。 《神雕侠侣》中出现鲜明宗派特点的是全真教和古墓派的故事。这里面尹志平和另一个人为了争夺全真七子后的掌教之位，不惜偷奸耍滑，干出各种没底线的事情，着实为这本书增添了不少色彩，相比于上一本书，这里的全真教就有更多宗派的味道在了，有了门派间的勾心斗角，长幼接替。这本书里的人物，比如小龙女还有李莫愁等，同是古墓派门人，因为古墓派的门规和武功而展开了一系列恩怨情仇，带上了鲜明的宗派烙印。更有趣的是这本书里的武功也有了宗派的烙印，古墓派武功轻灵，和外面的路数大相径庭；全真教武功被古墓派武功克制，但若是两人分别以两派武功齐心合力对敌又可发挥一加一大于二的效用。可以说，金庸小说里的宗派就是从《神雕侠侣》中开始的。 《倚天屠龙记》。《倚天屠龙记》中刻画了明教的势力分布，教主到各坛主堂主，等级分明，到后面甚至出现了明教总部，因为圣火令，总部和支部发生了冲突。《射雕》中有人物对九阴真经的争夺，《倚天》中是对倚天剑和屠龙刀的争夺，但这里人物行动的单位就发生了改变，不再是以“豪雄”个体为单位行动，而是以“宗派”为单位行动，对抗的单位也从个人升级为了宗派——这本书里的人大多是结队出行，虽说故事的主体还是部分豪雄，但这些豪雄不再洒脱，而是受“宗派”所限，比如武当山门人不得对峨嵋派门人出手，由此又衍生了不少故事。 囿于“宗派”和挣脱“宗派”的“豪雄”《天龙八部》我前阵子刚看完，印象比较深刻，可以更详细的分析实例。 慕容复：囿于“宗派”这是一个末路英雄的典型，不排除金庸小心眼恶意抹黑表哥（x）。 “北乔峰南慕容”，慕容家族享誉武林，但没人知道，慕容家一直都为了复兴鲜卑慕容氏而默默积蓄着力量。 慕容复是家族当代男丁，自然承担起了复国的重任。他玉树临风武功高强，身边追随了一大帮高手，还有美丽可人的王语嫣痴心相许，自然是妥妥的豪雄模板。 但他的故事总体是悲剧，从他身上一点也没有豪雄的洒脱，反而是被复国这个目标搞的心胸狭窄不成人样。（事实上《天龙》里没谁的故事不是悲剧，这本书的主旨就是“无人不冤，有情皆孽”“宿因所构，缘尽还无”） 他有了表妹王语嫣相伴，但为了复国想要与西夏公主联姻，不惜放弃表妹坑害段誉，甚至冤枉表妹变心于段誉，给人家搞的好不伤心，最后还真的自证预言，带上了好大一顶绿帽子（x）。 直到最后他一无所有，在故事的最后已经疯疯癫癫了，靠奴婢分糖给小孩，坐在土堆成的王座上，在段誉施舍的方寸之地里，梦中当上他心心念念的皇帝。这是我在这本书里第二喜欢的情节。 慕容复从头到尾都只是一台复国机器，可悲可叹。 萧峰：挣脱“宗派”萧峰是契丹人，其时宋人与契丹人势同水火。他的父亲萧远山，契丹人的大将军，致力于止战，但被小人（没错是慕容复他爹）陷害（没错还是为了复国），他就成了没爸没妈的孩子，被宋人领养，最后居然武功大成，当上了丐帮帮主。 到这个时候他还不知道自己的身世，一心报效祖国，恨不得生吞胡虏肉，渴饮匈奴血。但是突然有一天，他其实是契丹人的消息暴露了！又加上一堆栽赃陷害的阴谋，他一瞬间成了武林中众矢之的的大恶人。 这个时候他是无比迷茫的。一方面他从小归心于宋，但宋人都视他如仇雠；另一方面随着真相慢慢解开他发现自己的生父是个大英雄，但是被宋朝武人无端杀害，而那些宋朝武人又是他向来敬重的先辈；到后来终于有了契丹人皇帝与他结交，好像终于有了归心之所，但契丹人皇帝又逼迫他入侵他从小长大的地方，宋朝。 他崩溃了，故事的最后他自戕了。 这个人物的身份认同是非常困难并且复杂的，对于他只有阿紫身边是“应许之地”，但最后这片栖心之地也被他亲手打碎。这是我最喜欢的一个人物。他一生都被“宗派”所困，但即便如此他也丝毫不失自己的“豪雄”本色，始终有自己一直坚持的原则的底线。 《天龙》里我最喜欢的一个情节就是，萧峰大闹聚贤庄。 萧峰契丹人的身份暴露后，又有人把坏事栽赃于他，武林中众人就在聚贤庄中聚起来想对付他。这时萧峰失手打伤了阿紫想要救治她，得知神医也会在聚贤庄中出席。他不管这是一场商量着如何谋害他的“鸿门宴”，就这么径直闯入，牛饮美酒，“虽千万人吾往矣”，在聚贤庄里大杀四方，只为救助阿紫。 在这个情节里他不管自己到底如何被宋人白眼，他只想杀出聚贤庄，为了那个古灵精怪的女孩。 这看了谁不得叫一声好。 矛盾发生的维度这里给出了一张表格，按宏观到微观列出了矛盾发生的维度。 尺度 维度 宏观 家仇国恨，国家对立 介观 武林之中门派林立，争名夺利 微观 江湖男女恩怨情仇，轮转不休 看起来很完整了，可以囊括这四本书里所有的矛盾，但从其实逻辑上看这是不完备的，还少了一个。 缺失的第四个维度：庙堂在介观层面上，与江湖/草野相对的，自然是庙堂。但这四本书中，鲜有涉及。宋朝官府、蒙古官府里到底咋样的运作，里面又有啥故事，无人得知。即便是《天龙》中有大理皇室的描绘，但本质还是江湖儿女恩怨情仇那一套，什么段正淳到处拈花惹草啦，段正明才是皇室正统所以成了恶人啦，包括连皇室里有职位的那几个，都只是以武人的形象出现，而不是某个国家机关中的零件。这件事是很有意思的，不知道金庸接下来的几本书会不会有对这个维度的描绘。 对栽赃的分析：亲疏有别这里不得不提下让我有些审美疲劳的一点，这四本书中很多让人头皮发麻揪心无比的情节就是栽赃陷害。 “眼见不为实”，但偏偏每当某个英雄被小人陷害，平白套上好大一顶污名，就会瞬间变成众矢之的，真凶逍遥法外（当然最后都会水落石出），武林中的耋老名宿也瞬间降智，恨不得杀之后快。 我一度感觉这不合理——他们都是傻子不成？但细细一想这其实反应了一个亲疏有别的问题。 假如说小人A害死了B君，栽赃到英雄C上，耋老D瞬间降智，要杀了英雄C。这件事好像很不合理，事实上我们得这么来看： 对耋老D来说，这个B君较之英雄C，是D的至亲之人，比如说英雄C是他的徒弟，那也只能算D的次亲之人。想想看，在他看来（即便真相并非如此）次亲之人害死了至亲之人，他的反应肯定会恼羞成怒，加倍愤怒，直呼好啊你个白眼狼，恨不得杀D而后快——这太合理了！假如英雄C和耋老D并无瓜葛，D可能都不会这么愤怒。 但如果B君和英雄C对耋老D的地位呼唤，至亲之人杀了次亲之人或者说毫无瓜葛之人，相信除了郭靖这个憨憨还是会选择大义灭亲，其他人一定都会三思而后行，不愿相信至亲之人会干出这种事。 所以栽赃陷害中受伤的总是英雄，这也确实没啥办法，就算是他老妈也有兄妹，可能比他更亲呢。 “满怀愧疚的活下去吧”终于到标题了。 整篇博文只有这最后一小部分和标题有关系，未免有诈骗之嫌，但没办法，标题还是得放上最漂亮的句子（x）。 有这种思考是因为，武侠小说里的儿女基本个个胸怀仁义道德，为了贯彻这些行事准则即便一死也无妨。比如《天龙》中大理皇室的那个官员，段正淳的随从，武功高强但是被段正淳的私生女（段正淳有好多私生女）用奇兵戏弄，他不堪折辱，但又因为羞辱他的是主子的女儿，无法排解，最后自杀式的死在了和对头的拼杀中；再有后面段正淳的几个情人都被害死了，于是他也选择殉情，段誉他妈一看不行，跟着一块死了，好哇落的大地白茫茫一片真干净，段正淳剪不断理还乱的情人网总算全没了，就是搞的段誉一个人孤零零的，为了大理段氏后继有人，还是顽强的没选择去死。 其实写的挺好的，人物形象这不都塑造起来了吗，但我想，假如我遇到这种情况，我会为了贯彻内心的某种坚持，用死亡去终结一切吗？ 后来我看了《逃避虽然可耻但是有用》。gakki和gen桑“事实婚姻”，好算给家人有了个交代，但毕竟不是真正的情投意合的情侣，还得瞒着亲朋好友。 有一集，gakki受不了了，对gen桑要不我们对yuri酱（剧里gakki的小姨）和盘托出算了，这样瞒着她我心里好愧疚！gen桑的回答我至今难忘：如果我们对yuri酱和盘托出，我们是不用承担道德压力了，但是yuri酱呢？她因为爱你就会优先成全你,替你保守秘密；但相对的她，在你妈妈面前就要撒谎，我们其实只是把道德压力转移到别人身上了，这样真的好吗？ 豁然开朗。 也终于理解了善意的谎言到底意味着什么。很多时候，和你爱的人相比，一些固执的坚持没有必要。 所以为了你爱的和爱你的人，满怀愧疚的活下去吧。你一定还是他们的光。"},{"title":"机器学习-03-some mathematical perspectives(1)","date":"2022-03-30T08:12:44.000Z","url":"/2022/03/30/post18/","tags":[["machine learning","/tags/machine-learning/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"本文事实上为应用数学讨论班的部分课堂笔记。解释神经网络的泛化性等等都会用到更多的数学工具。这篇博文持续更新。 ai for science这可以说是整个应用数学讨论班讲的事和鄂维南院士正在大力推动的事情。 multi-scale model据ewn所授，科学计算用到的所有模型都可以归为7类，从微观到介观到宏观分别是 QM(quantum mechanics)，量子多体模型，用薛定谔方程ab initio计算。这样做是最准的也是计算代价最大的。是唯一一个不需要model input的模型。 DFT(density function theory)。密度泛函理论。有一个人指出体系基态能量与一个电子密度的泛函相对应，所以DFT做的事情就是把关注点移到电子密度而非波函数，这样做的好处在于减少了变量（维度）。比较不好的一点在于需要model input，也就是交换关联泛函（$V_{exc}$）。这个方法的计算复杂度是$N^3$，但不如ab initio准，问题在于$V_{exc}$的确定是很难的。不过所幸对于大部分问题$V_{exc}$的选取都对要研究的量不会有太大影响，除了半导体。 MD(molecular dynamics)。把原子简化为刚体球，点电荷，然后粒子对对碰算体系。需要input粒子potential，过往这个potential的确定往往是通过猜，也未必好用。 course grained MD。粗粒化，一些体系往往由相同性质的部分组成，比如蛋白质液晶，这时候做MD没必要单独考虑每个原子，将一些原子视作整体进行MD计算，方便许多。同样需要input potential。 continuum mechanics。连续介质模型，比如流体力学弹性力学，这也是我的老本行（x）。其实感觉连续介质力学和粗粒化很像，考虑的是物质微团，这些微团宏观足够小微观足够大，一方面保证了可以用分析的手段去建模处理，一方面保证了每个微团的性质大体相同，保证了数学处理的合理性。建模时用经典力学的动量定律守恒律等获得控制方程。这里同样需要model input，是本构关系。 kinetics equation。动理学方程，以玻尔茨曼（Boltzmann）方法为代表，多用于航天，考虑稀薄气体中原子的碰撞，属于介观尺度的数值模拟方法。这里的model input是表征碰撞的函数。 turbulence。湍流，不多说了，青年科学家的坟墓（x）。where the pigs don’t fly。 application of machine learning for scientific computing由前所述，这些科学计算模型都需要有先验的模型输入，大部分情况下确定这些输入都是困难的，在机器学习出现之前确定这些输入都需要用猜蒙碰凑的方式，包括图像处理中，一代代数学家可能一辈子研究（猜）出几个好用的特征就满意了，就是说这里的handcraft是非常difficult的。然后，here comes the machine learning。 这里的机器学习都指supervised learning。直观的说机器就是一台超级插值机，利用它可以解决上述问题中model input难以手工获取的困难。例如MD，就是一个机器学习应用的非常成功的例子。这里用DFT生成小规模的高精度数据，用来train粒子的potential，效果是非常好的。 这门课让我感觉非常开心的一点也在这里。我越发感觉认知到整个知识体系，overall landscape，并且明白自己所学所研究的东西到底在这个overall landscape的什么位置，这件事是非常重要的。流体力学归属于连续介质力学模型，而连续介质模型不过是七大科学计算模型中的一个而已，机器学习也并不是神乎其神无所不能，只是刚好在算力发展的背景下可以取代过往高昂的handcraft，一件趁手的工具罢了。 精彩的东西还有好多啊。 再看优化都说机器学习其实就是在解一个优化问题，但其实二者的区别是很大的。 比如说，Given $S = {(x_j,y_j=f^*(x_j))},learn \\ f^*$,机器学习相当于优化loss function $$ \\operatorname{arg\\,max}_w L(w,x,y) $$ 使得$f(x)$逼近$f^*(x)$ 事实上这只是针对train data，采样获得的、“看的见”的数据做优化，优化的只是”empirical risk”$$\\hat R_n(f) = \\frac{1}{n}\\sum(f(x_j)-y_j)^2 = \\frac{1}{n}\\sum(f(x_j)-f^*(x_j))^2$$但我们真正关心的，是这个网络是否具有泛化性，是否拿来推断时也有好的表现，也就是在“没看见的数据”上也接近$f^*(x)$，即真正要优化的目标是”population risk”（即generalization error）$$R(f)=E_{x\\sim \\mu}[f(x)-f^*(x)] = \\int(f(x)-f^*(x))^2d\\mu$$有很多手段都可以优化empirical risk，但population risk也同样小，即得到有良好泛化性的网络是难的。 over parameterized用解方程的视角来看待训练神经网络，给定$n$条训练数据相当于给定了$n$个方程，假如神经网络有$m$个参数，也就是说用$n$个方程来解$m$个未知数，解显然不是唯一的，解在一个$dim = m-n$的manifold上。 那么这样看，训练神经网络的过程就相当于是用某种方式让一组参数落在这个$m-n$维的流形上，每组参数可以决定一个神经网络的样子，这个神经网络在样本处都足够接近$f^*(x)$。但是真实数据的分布$f^*(x)$是唯一的啊？这就导致train出来的神经网络在样本点之外的值，也就是做inference时的表现千差万别。 这样就为理解过拟合提供了一种新的视角，train出来的参数分布在$m-n$的流形，如果恰好train出来一组参数有比较好的表现，那是很难得的啊！所以会有正则化对参数提供更多约束。 to be continued…"},{"title":"机器学习-02-理解正则化的三种视角","date":"2022-03-29T14:11:11.000Z","url":"/2022/03/29/post17/","tags":[["machine learning","/tags/machine-learning/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"看李宏毅的机器学习网课终于知道了正则化在干嘛，又为了加深理解去网上找了几篇文章看。这篇博文提供了直观、优化和概率三种视角来理解正则化。 正则化大略正则化就是在训练时引入对参数的先验估计，起到一个约束的作用。 介是啥regularization，在原loss function中加上一项，这一项往往是参数的范数，来辅助优化，又叫惩罚项，有控制模型复杂度，防止过拟合的作用。 有哪些 L1正则化。正则项是1-范数，loss function里加上$|w|$，为参数绝对值之和。 L2正则化。是2-范数，加上$\\sum w^2$，为参数平方之和。 干了啥 L1正则化，让Loss function在训练中不少参数能gradient descent到0，即最后让参数具有稀疏性。 L2正则化，训练过程中参数能趋向0，变的很接近0（往往不是0）。 有啥用 L1正则化。学习过程中参数捕捉了数据的特征，但不少特征是对目标无用，参数稀疏化相当于丢弃了这些无用的特征，再保证train过程中loss小的同时，inference阶段这些无用特征不会产生干扰，提高了推断效果，降低了过拟合的可能性。 L2正则化。forward pass中$y =w^Tx$，我们知道$\\frac{\\partial y}{\\partial x} = w$，L2正则化让$w$变的很小，趋向0，即能让输出值对输入值的导数很小，效果就是输出值对输入值的改变不敏感。直观的讲，就是做到让train出来的model足够smooth，而这样做的原因是我们往往认为真实数据的分布规律也是smooth的，这样做就防止了过拟合，使得model在除了train dataset的点上也能更接近真实分布。专业一点说，就是这样使得model抗扰动能力强，要知道采样得到的数据往往是会有噪声存在的，L2正则化就是增强model对这些噪声的抵抗能力。 从优化的观点看正则化以监督学习为例，本质上我们做机器学习就是解优化问题 $$ \\operatorname{arg\\,max}_w L(w,x,y) $$ 其中$x$是data，$y$是Label。 正则化：拉格朗日极值从优化的观点看，正则化就是给$w$的优化，根据对参数的先验估计（如稀疏、足够小），引入了约束条件， L1正则化。$s.t.\\begin{Vmatrix}w \\end{Vmatrix}_1 \\leq C$ L2正则化。$s.t.\\begin{Vmatrix}w \\end{Vmatrix}_2 \\leq C$ 即对1-范数，2-范数的约束。 利用拉格朗日乘子法可以构造不带约束的极值问题，即 $$ \\operatorname{arg\\,max}_w [L(w,x,y)+\\lambda(\\begin{Vmatrix}w \\end{Vmatrix}_{1\\ or\\ 2}-C)] $$ 此即带正则化项的loss function，求解该优化问题等价于上带约束条件的优化问题。C是控制的，一般取成0。 从最大后验估计（MAP）看正则化讲道理机器学习本来就是统计学家的领地。概率是已知模型和参数（生成数据的方式），去看数据的样子，统计干的事情是根据数据分布（采样），推测参数和模型。机器学习不就是从数据里学分布嘛，所以机器学习的大部分理论都是建立在统计理论上的。 概统里都学过最大似然估计（MLE），意思是有了一堆数据，又知道了模型，现在要估计模型参数，MLE干的事就是找到参数，这组参数使得样本数据出现的概率是最大的。想法是很好的，最大概率出现的最有可能被采样到，但也不是尽善尽美，比如抛十次硬币很难保证五次向上五次向下，这时候做MLE往往能把正反面出现的概率估计的很离谱。 最大似然估计也就是知道了,例如$$y\\sim N(f(w,x))$$中$f$的形式，有了一组$(x_i,y_i)$样本，令$$log \\Pi P(y_i|x_i;w)$$取最大值。 想了解最大后验估计，必须得了解贝叶斯。 贝叶斯大略贝叶斯统计的核心是，参数同样随机，而且满足一个分布。 由贝叶斯公式（这个公式我拿来就用的，并不会证明）$$P(w|x,y) = \\frac{P(w,x,y)}{P(x,y)} = \\frac{P(x,y|w)P(w)}{P(x,y)}\\propto P(y|x;w)P(w)$$ 最大后验估计后验估计的意思就是说，我们已经知道了$w$参数的分布，是先验的信息，我们利用样本$(x_i,y_i)$的后验信息，估计参数的值。 比如说我们知道参数服从高斯分布$$w \\sim N(0,\\sigma ^2)$$由贝叶斯公式，最大后验估计就是优化$$logP(y|x;w)P(w)$$其中$$logP(w) = -\\frac{1}{2\\sigma^2}\\sum w^2 + C$$即MAP让最大化的目标加上了这么一项，可见引入高斯分布的先验估计进行MAP等价于引入L2正则项。L1正则项等价于先验估计参数服从拉普拉斯分布。"},{"title":"图像处理-01-从滤波器到pde-net中的微分算子","date":"2022-03-28T12:30:38.000Z","url":"/2022/03/28/post16/","tags":[["图像处理","/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"董彬老师的pde-net整了个好活，从图像处理的滤波器引入微分算子，用于近似pde中的微分。这篇博文就是学习滤波器的笔记。 滤波器101这是图像处理中的概念，做了一件什么事呢，滤波器作用于某个像素，将这个像素的值变为周围像素的线性组合。$$g(x,y) = \\sum_{-a}^{a}\\sum_{-b}^{b}w(s,t)f(x+s,y+t)$$$g(x,y)$是$(x,y)$处像素变换后的值，$w(s,t)$是线性组合系数也就是滤波器，$f(x+s,y+t)$是与滤波器系数对应位置的原图像像素值。 设计不同的滤波器系数可以起到不同的图像处理效果，这篇博文中不赘述。 微分算子学过数值方法的大家都知道数值微分中拉普拉斯算子的离散格式，在图像处理中 在分析中是在$x,y$上加小量，在图像处理中都对像素处理，最小单位是固定的一个像素，作为某种index这里就在$x,y$上加1了。 有这种求和规则，对应的滤波器是 第一个就是与上面离散格式相对应的滤波器，其余几种是为了满足特定应用场景提出的，如第二种强化了中心点，提高了点检测的效果。 对于一阶微分，也可以提出类似的滤波器 一般来说一阶微分只涉及两个量，但也可以用三维的量去离散，分析意义下等价的；图中的sobel算子中线上系数是2，目的是加强边缘线的检测。 高阶微分 想求高阶微分就需要更广的区域，上图是计算牛顿的高阶均差示意图，实际上与数值微分离散格式有异曲同工之妙；高阶均差等于两个相邻低阶均差的均差，如此层层加码，计算$n$阶微分就需要$n+1$维的数据了。 滤波器？求和规则！在董老师的pde-net中有这么一个命题，当然我看的不是很懂，呜呜 这里的$k,\\beta$都是二维的整数组，$(2)$好像定义了一种在某种求和规则下得0的滤波器，然后就定义了这种滤波器的阶；$(3)$好像就说，这种滤波器作用于一个二元函数可以视作某种微分… 通过上面一节或许可以对这个命题有直观的理解；数值微分告诉我们对于任意阶的微分肯定是存在这样一个滤波器和它对应的，毕竟滤波器其实也只是一种求和规则的记法（notation）。董老师应该是非常严格的叙述了这一intuition，但我看不懂… 然后在这里有$frozen$和$unfrozen$的取舍，就是微分算子滤波器使用时可以完全固定下每一个值（$frozen$），也可以放开其中几个自由度，固定的那些值确保它至少满足几阶微分。 反正就，有了这个命题就可以把这个引入下面pde的讨论了。 pde-netnn拟合微分算子这里的$(x,y)$是2维场均匀离散化的角点，通过如此差分，获得了一个好比像素图的场，这里场的值$u(x,y)$就好比图像处理里的像素，图像处理里的微分算子到这里也就成了正儿八经的微分算子。 这里使用的微分算子都放开了自由度，不是全frozen的，就是说这里的算子是data-depending的，其中部分参数训练得到。 网络架构然后是这篇文章非常有意思的一个地方，网络架构如下图，需知本文的网络架构是非常非常特殊的，和多层感知机之流截然不同 这个网路的参数分别有滤波器的参数和神经网络的参数，如果我没理解错的话，神经网络的作用就是单纯的将经微分算子作用的值和其他项的值组合在一起，引入先验的假设，这样的组合是有限的，那么之后的就是这些组合项的线性组合。 所以这个网络的架构是相当透明的，训练完之后通过check神经网络的参数就可以换算出pde控制方程中的各项参数，如果参数很接近于0，那么可以认为这一项在真实控制方程中是不存在的。这就做到了”learning pdes from data”，训练完之后的网络不仅可以推断pde的演化，也可以告诉我们数据满足怎样的控制方程（pde）产生的。 一些细节 上图展示的架构叫一个$\\delta t \\ block$ ，实际网络是由好几个$\\delta t \\ block$组合而成的，这些$\\delta t \\ block$共享神经网络参数，事实上是在说每个时刻都满足一样的控制方程，这也算一种先验信息了。 组合起来的效果就是，一个$\\delta t \\ block$算出的$\\hat u(x,y)$传入下一块继续训练，我没理解错的话训练是一层一层单独进行的，自用frozen filter的预热阶段得到了神经网络参数后，每层的训练都只train滤波器中的参数了。 如此train的目的是让这个网络可以有良好的推断效果。 这个网络没法自动微分，只能数值微分。 "},{"title":"弹性力学-04-应变能密度的故事","date":"2022-03-27T18:17:29.000Z","url":"/2022/03/28/post15/","tags":[["弹性力学","/tags/%E5%BC%B9%E6%80%A7%E5%8A%9B%E5%AD%A6/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["我来我见我征服","/categories/%E6%88%91%E6%9D%A5%E6%88%91%E8%A7%81%E6%88%91%E5%BE%81%E6%9C%8D/"]],"content":"弹性力学上到了本构关系和边值问题。内容多且繁琐，但笔者很惊喜的发现，应变能密度在这两部分里都扮演了很重要的角色，也有着很鲜明的物理意义，所以争取在这篇博文里用应变能密度把重要的内容串起来。写完了发现这是我写过最长的一篇博文，也是花了最多心思的一篇。 其实回过头来看里面的一些insight不难，只要授课老师点一下，学生就能明白。但是魏院士做不到。所以学生只能花大量的时间和心思找各种资料，来获得对教材内容的理解。可能也是，用学长的话来说，《弹性力学教程》全世界除了王敏中和黄克服没人能讲明白了。 知识框架 由变形过程中由位移变分推导出外力、内力做功变分公式，和在不同热力学条件下应变能密度对应的热力学量，后者是平凡的。 由做功变分公式，应变能密度对位移的偏导为应力，小变形下对应变能密度泰勒展开，得到应力和应变的线性关系。这就是广义胡克定律。E是弹性常数。 应变能密度应当具有正定性，正定的性质在弹性力学边值问题的解的唯一性中起到了作用。（本质上就是能量积分证明pde唯一性） 弹性势能本质上就是内能加外力势能，可以对位移变分推导出最小势能原理，对应虚功原理。最小势能原理在运动学许可状态下给出，本质是是给出了$\\delta s$的约束，这种许可状态下$\\delta s$遍历弹性力学边值问题中可能出现的位移。 弹性余能是让应力变分，对应虚力原理。 弹性势能和弹性余能二者本质都是势能——内能与外力势能之和极小值原理，不同的变分对象导致了不同了表达式。 变形时外力（面力体力）做功 $$ \\begin{cases} \\nabla \\cdot T + f =0,(\\Omega)\\\\ n \\cdot T = t,(\\partial \\Omega)\\\\ \\end{cases} $$ 这是弹性体平衡方程，想要计算弹性体从$u$变形到$u+\\delta u$外力做的功，$$\\delta K = \\int_{\\partial \\Omega}t \\cdot \\delta uds+\\int_{\\Omega}f\\cdot \\delta ud\\tau$$应用平衡方程$n \\cdot T = t$和奥高定理，处于合并同类项的考虑，原式子化为$$\\delta K = \\int_{\\Omega}(\\nabla \\cdot T + f)\\cdot \\delta ud\\tau+T: \\delta u\\nabla d \\tau$$这里可以发现一个有用的分部积分恒等式$$\\nabla \\cdot(T \\cdot u) = (\\nabla \\cdot T)\\cdot u+T: u$$ 变形过程中外力做功为$$\\delta K = \\int_{\\Omega}T : \\delta u \\nabla d\\tau$$由$\\delta u \\nabla$的对称反对称分解双点乘的性质和$T$的对称性，对称张量和反对称张量双点乘结果为0，得到$$\\delta A = T: \\delta \\Gamma$$这里的$\\delta A$是单位体积上外力做的功，全区域上积分就是弹性体所受的功。 广义胡克定律由热力学定律，物体内能（总能量）为外界输运的热能和外界机械做功之和，$$\\delta U = \\delta Q+\\delta A =\\theta \\delta S+T: \\delta \\Gamma$$绝热条件下可以提出$$T_{ij} = \\frac{\\delta U}{\\delta \\Gamma_{ij}}$$用勒让德变换，在等温条件下可以得到$$T_{ij} = \\frac{\\delta F}{\\delta \\Gamma_{ij}}$$分别适用于变形快，热量来不及交换和变星慢，热量充分交换的过程。 我们把$U$或$F$统一叫做应变能密度$W$，由上面的关系，$\\Gamma$为小变形，对$W$泰勒展开，引入无初应力假定，可以干净的得到本构关系，也就是广义胡克定律$$\\sigma_{ij} = E_{ijks}\\gamma_{ks}$$$E$为弹性常数，是完全对称的四阶张量$$E_{ijks} = \\frac{\\partial^2W}{\\partial \\gamma_{ij}\\gamma_{ks}}$$求导可交换。 在完全各向同性下，可得$$T=\\lambda J(\\Gamma)I+2\\mu\\Gamma$$$\\mu$和$\\lambda$叫Lame（拉梅？）常数，前者也叫剪切模量（和工程对上了？）。 由$$\\delta W = E_{ijks}\\gamma_{ij}\\delta\\gamma_{ks}$$以及$W$对称性得$$W = \\frac{1}{2}E_{ijks}\\gamma_{ij}\\gamma_{ks}$$这个式子非常重要。 到这里我们就得到了应变能的表达式，是一个二次型，由内能、自由能在稳定平衡处平衡处取极小值（这些都是在热力学中有严格证明的）知$W$正定，故得到Lame系数的约束$$\\mu&gt;0,3\\lambda+2\\mu&gt;0$$本构关系也可以写成$$\\Gamma=\\frac{1}{E}[(1+\\nu)T-\\nu J(T)I]$$其中$$\\mu = \\frac{E}{2(1+\\nu)},\\lambda = \\frac{E\\nu}{(1+\\nu)(1-2\\nu)}$$$E$和$\\nu$满足$$E&gt;0,-1&lt;\\nu&lt;\\frac{1}{2}$$ 弹性力学边值问题$\\Gamma,T,u$满足几何方程，平衡方程，本构方程 $$ \\begin{cases} \\Gamma = \\frac{u\\nabla+\\nabla u}{2}\\\\ \\nabla \\cdot T+f=0,(\\Omega)\\\\ T=\\lambda J(\\Gamma)I+2\\mu\\Gamma \\\\ \\end{cases} $$ 这组Pde有15个变量，15个标量方程。 对这个方程可以提边界条件 $$ \\begin{cases} u= \\bar u(\\partial_u \\Omega)\\\\ n \\cdot T = t,(\\partial_t \\Omega)\\\\ n \\cdot T +ku=0,(\\partial_e \\Omega) \\end{cases} $$ 第三种弹性边界条件本质和第二种应力边界条件并无不同，不过规定了面力的形式，事实上非常特殊，后面规定运动学许可状态和静力学许可状态时，这个边条的导致的外力做功项总存在，而前两个边条未必。这个在后面细说。 边值问题的唯一性其实就是证明pde的唯一性，证明它的齐次问题仅有平凡解（零解）。蔡庆东老师的数学物理方法（下）课上介绍了能量积分的方法，就是未知量各导数项的平方之和，在弹性力学问题中我们很方便的可以找到这个能量——$W$。$$2\\int_{\\Omega}Wd\\tau = \\int_{\\Omega}T:\\Gamma d\\tau = \\int_{\\Omega}T:u\\nabla = \\int_{\\partial \\Omega}n\\cdot T\\cdot uds-\\int_{\\Omega}(\\nabla \\cdot T)\\cdot ud\\tau$$推导过程依次用到了双点乘性质，奥高公式，分部积分公式。然后依次带入边界条件化简得到$$2\\int_{\\Omega}Wd\\tau = -\\int_{\\partial_e \\Omega}ku_iu_id\\tau$$而$W$正定，故$W=0$，齐次方程仅有零解，边值问题解唯一。 总势能的变分这一块非常重要！！！可以说是整篇博文的精华所在了。变分推导部分请教了武鑫鑫学长。 考察$$s=[u,\\Gamma,T]$$这一组值完全确定了一个弹性力学问题，叫做一个弹性状态。 弹性体内能变化有赖于外力做功，即$$\\delta U = \\int_{\\partial \\Omega}t\\cdot\\delta uds+\\int_{\\Omega}f\\cdot\\delta ud\\tau$$外力做功同样存在一个势，故我们考虑外力做功（变形）过程中的总势能的变分$$\\delta \\Pi(s) =\\delta U - \\int_{\\partial \\Omega}t\\cdot\\delta uds-\\int_{\\Omega}f\\cdot\\delta ud\\tau = 0$$把变分符号提到最外面，得$$\\delta \\Pi (s) =\\delta[U -\\int_{\\partial \\Omega}t\\cdot uds-\\int_{\\Omega}f\\cdot ud\\tau] = 0$$外力作用下变形过程中总势能的变分为0，考虑二阶变分的话可以得出： 真实情况下弹性体内能与外力势能之和取极小值，这就是最小势能原理。 其实上面的讨论并不严谨，但我们得到了一个很重要的量，总势能（弹性势能），和最小势能原理。$$\\Pi (s) =U -\\int_{\\partial \\Omega}t\\cdot uds-\\int_{\\Omega}f\\cdot ud\\tau$$在给定弹性力学边值问题下，真实弹性状态令这个量取极小值。 下面，我们在给定边界条件的情况下对位移变分，进行推导。$$\\delta \\Pi(s) = \\int_{\\Omega}\\delta W d\\tau -\\int_{\\partial \\Omega}t\\cdot\\delta uds-\\int_{\\Omega_t}f\\cdot\\delta ud\\tau$$其中，由双点乘性质和几何方程$$\\delta W = T:\\delta \\Gamma = T:(\\delta \\Gamma+\\delta \\Omega) = T:\\delta \\nabla u$$这里的$\\Omega$是反对称部分，区别上面出现的积分区域$\\Omega$。 然后用上面提到的分部积分公式$$\\delta W = \\nabla(T\\cdot \\delta u)-(\\nabla \\cdot T)\\cdot \\delta u$$回代入上面的积分$$\\delta \\Pi(s) = \\int_{\\Omega}[\\nabla(T\\cdot \\delta u)-(\\nabla \\cdot T)\\cdot \\delta u] d\\tau -\\int_{\\partial \\Omega}t\\cdot\\delta uds-\\int_{\\Omega_t}f\\cdot\\delta ud\\tau$$由奥高公式$$\\int_{\\Omega}\\nabla(T\\cdot \\delta u)d\\tau = \\int_{\\partial \\Omega}n\\cdot T\\cdot\\delta u ds$$这里的$\\Omega = \\Omega_u+\\Omega_t$，分别拆开代入化简$$\\delta \\Pi(s) = \\int_{\\partial\\Omega_t}(n\\cdot T-t)\\delta uds+\\int_{\\Omega}(\\nabla \\cdot T-f)\\delta ud\\tau =0$$如此竟得到充分条件：控制方程，即给定边界条件、几何方程下，弹性状态必须满足的条件。 $$ \\begin{cases} \\nabla \\cdot T + f =0,(\\Omega)\\\\ n \\cdot T = t,(\\partial \\Omega)\\\\ \\end{cases} $$ 推导时刻满足充分必要性，除了最后控制方程是充分条件。 那么我们可以说，给定几何方程、边界条件，如果一个问题满足控制方程，即对于一个弹性力学边值问题的真实解，也就是给定边界条件下的真实弹性状态，使得弹性势能最小。 运动学、静力学许可状态考察$$s=[u,\\Gamma,T]$$这一组值完全确定了一个弹性力学问题，叫做一个弹性状态。 如果说$s$满足几何方程、本构方程和位移边界条件，我们把它叫做运动学许可状态，事实上表示$s$的变分不是在全空间进行，而是存在约束，$\\delta s$在一个子空间上进行，这个子空间主要限定了运动学的状态（位移边条，几何方程给定了$u$和$\\Gamma$的关系，本构关系给定了$\\Gamma$和$T$的关系），使得$s+\\delta s$遍历了可能出现的位移（在这个约束之外的位移一定是不可能的，好比如果不满足位移边条那这个位移不可能是真实解）。 假如$s$满足平衡方程、应力边界条件、弹性边界条件，我们把它叫做静力学许可状态，事实上也是约束了$\\delta s$所在的子空间，限定了静力学相关的量的状态，使得$s+ \\delta s$遍历了可能存在的内力（假如不满足应力边条那这个应力不可能是真实解）。 最小余能最小余能在静力学许可状态下讨论。 书上应力应变所表示的应变能看起来是一样的，其实这二者天差地别。因为这本书在线弹性下讨论，事实上前者叫变形势能，后者叫变形余能，代表$\\gamma-\\sigma$曲线下部积分和上部积分，即取微元片时固定应力不变还是应变不变，对于线弹性，曲线是直线，故势能等于余能。 再来一些直观阐述：势能在位移的变分下有极小原理，此时总能量（弹性势能）表达式为应变能减外力（面力体力）做功；余能在应力变分下有极小原理，表达式理应则是余能减外力做功，此时固定位移不变，外力做功由应力变化引起，而体力是外力，我们变分时不改变体力，所以只有$n \\cdot T = t$中的面力$t$改变引起外力做功。 弹性余能最小原理对应虚力原理。 到此intuition地阐述了弹性势能和弹性余能的区别和原因。"},{"title":"01-再次欢迎","date":"2022-03-26T08:47:26.000Z","url":"/2022/03/26/post14/","tags":[["重大说明","/tags/%E9%87%8D%E5%A4%A7%E8%AF%B4%E6%98%8E/"]],"categories":[["这个男人来自地球","/categories/%E8%BF%99%E4%B8%AA%E7%94%B7%E4%BA%BA%E6%9D%A5%E8%87%AA%E5%9C%B0%E7%90%83/"]],"content":"博客建设了一个多月，重新明确了自己建设博客的目的和路径，所以，“再次欢迎”。关于第一次欢迎，可以参见00-欢迎。 Trivialization私以为，认知水平的高低在于能否把比较困难的事情“平凡化”，and take them for granted。 这是在学习一些定理的证明的时候发现的。大部分定理都是由一个两个最核心的insight支撑起来的，把握了这些insight也就把握住了这些定理证明的脉络。有时候这些insight令人难以接受，即便可以看懂也无法完全接受，但对于大佬，他们往往会认为“就该是这样啊”“这不是很平凡的事情吗”，看了一遍证明就可以自己独立的把它写出来。 我觉得认知水平的差异就体现在这里了：能否快速把一件事物内化为自己的储备，关键在于能否快速把晦涩的事物trivialize。想做到这一点需要对这件事有深刻的认知，理清所有条理并内化，才能清楚的把它说出来。 高低差异在于，可能对于百分之八九十的定理大佬们都可以快速trivialize，但我可能只能cover一半不到的定理，剩下的都得我反复咀嚼才能做到trivialize。我想费曼学习法的作用可能就在这里，要做到能把一件事情清楚的讲给别人听是需要做到trivialize的，倒逼自己做深刻认知。 而写博文就是这样一件事。我只有深度理解某个知识，才能做到不依赖或者轻度依赖书本就把一个某个知识的关口和脉络讲清楚。而这些关口和脉络也是理解一知识的过程中，我认为不那么trivial的地方，也就是我当下认知水平尚不能及之处。假如说我认为某个知识全部都是很平凡的，那就不会写出来了2333。 希望可以通过写作，把一件件事情都做到trivialization，拾级而上，慢慢提高自己的认知水准。 归档记录下每个阶段自己零碎的想法，然后在未来几年之后一一回顾，相信会非常有趣。 人的思想是连续变化的，但大部分时候我们只能看到当下的状态，过去的状态也是很珍贵的呀！所以要坚持记录下一步一步走来的过程。有些有趣的想法也不能轻易丢掉，哪天我失去了想象力，我就得靠这个博客过去的记录过活了23333。 一期一会，会者定离很多说再见的时候，都是和那个人见的最后一面了。从那之后他们就在我的世界里一点点消失，一点点死去。我在他们的世界里亦然。"},{"title":"十六日谈-01-他人的舞台与不灭的英雄梦想","date":"2022-03-14T17:51:24.000Z","url":"/2022/03/15/post13/","tags":[["十六日谈","/tags/%E5%8D%81%E5%85%AD%E6%97%A5%E8%B0%88/"]],"categories":[["这个男人来自地球","/categories/%E8%BF%99%E4%B8%AA%E7%94%B7%E4%BA%BA%E6%9D%A5%E8%87%AA%E5%9C%B0%E7%90%83/"]],"content":"$$人没法两次踏进同一条河流，但我在每个月的十六日都可以进入同一条沈阳大街。$$经济学上的二八定律说的是，世界上20%的人占有了80%的财产。但事实上二八定律在相当广的范围里都成立。就比如我可以这么说，世界上20%的人占有了80%的经验，剩下的80%占有剩下贫瘠的20%。 “再生产”“剩余价值”“学位”“被引量”“阅读量”……我们的意义好像被绑定在冰冷的指标上，个性被消解在词语与数字的漩涡里。在日复一日的劳作中我们的生活日益干瘪，人被异化为这台社会的零件。 但没有人想这样。于是我们把塑像放进神龛，在一次又一次仪式中迎来每天都准时开场的盛大节日。把日常生活颠倒为节日庆典的神圣仪式。 在这里我无意叙述嘉然为什么是神，但我想向你们隆重介绍我的精神家园(x)——沈阳大街。这里是浪漫与梦想的故乡，坍圮了巴比伦塔，却也埋下了光荣与希望的种子。 我们永远也没法成为虎哥、刀哥，没法加入杀马特团，没法随时随地在路上来一个后空翻。我们失去了成为英雄的能力，故事与史诗仅仅成为口耳相传的赛博幽灵，至少我不知道沈阳大街在哪，我也无意去了解。但是没有关系。 沈阳大街是浪漫的，浪漫是包容的。在贫瘠的日常劳作中，我们迫切的寻求浪漫去充盈生活。大部分时候我们不得不用一段故事去讲述浪漫，把浪漫绑定在各式各样的景观上。通过这一过程一件件本无特殊意义的表象的集合构成了某种宏大节日庆典的一部分——“我们联合！”，于是浪漫主义整活机器的齿轮开始转动，我们把生活中破碎无意义的场景与那个浪漫令人心驰神往的戏剧结合起来。在他人搭建的舞台上，我们这些人也能翩然起舞。 所有人都可以在沈阳大街高呼：“我是个傻逼！”或是终于找到独属于自己的句子。 言语加速了交流，同样也林立壁障。这几乎是无法避免的事，不管在何时一句顶一万句都是人永恒的追求与愿景，但依赖声带振动介质传播的交流方式确实低下。 但在沈阳大街不会。你总可以在这里找到击穿你内心的画面和声音。浪漫是抽象的，在这里我们可能无法听懂对方说的每一句话，但又扎扎实实的理解了一切。 所以，今天几号？ $$ we \\ can \\ be \\ heroes, \\ just \\ for \\ one \\ day $$"},{"title":"张量-03-弹性力学中的分量变换","date":"2022-03-13T14:39:15.000Z","url":"/2022/03/13/post12/","tags":[["张量","/tags/%E5%BC%A0%E9%87%8F/"],["弹性力学","/tags/%E5%BC%B9%E6%80%A7%E5%8A%9B%E5%AD%A6/"],["线性代数","/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"一般的坐标变换在张量-02-协变逆变 中有提到比较一般的坐标变换。 旋转坐标变换比较特殊的一点是，此时的变换矩阵是单位正交阵。就是说，如果$$c_{ij}\\hat e_i = e_j$$一定有 $$\\hat e_i = (c^{-1})_{ij}e_j$$但因为单位正交阵有$C^TC=I$，所以有 $$\\hat e_i = C_{ji}e_j$$那么二阶张量的坐标变换可以写成 $$\\hat A_{ij} = C_{ki}C_{sj}A_{ks}$$ 应变张量的坐标变换由这篇弹性力学-02-应变分析，所以$\\xi$上的应变率为（小变形时） $$\\epsilon = \\xi_i \\gamma_{ij}\\xi_j$$我们提出$$\\hat e_i = C_{ij}e_j$$得到$$\\hat \\gamma_{ij} = C_{ik}C_{js}\\gamma_{ks}$$$\\Gamma$是对称张量，这里$C_{ij}$的提法也和上面不一样，两种想法导出的是同一种东西。 但是有一个问题，任意方向上应变率的结论是在小变形的条件下适用的，那么应变张量的坐标变换是否也在小变形下成立？可是单纯从坐标变换角度出发那个式子是总可以推导出的，或者说，只有在小变形下这个变换公式才是有实际意义的？ 感觉是后者，只要有张量，分量变换公式就总是成立的，而对于非小变形情况，应变率就是非常面目可憎的，也没法简洁提出类似于单个张量分量变换的形式。 应变张量的分量变换同样有$$\\hat e_i = C_{ij}e_j$$由应力分量定义$$\\hat T_{ij} = (\\hat e_i \\cdot T)\\cdot \\hat e_j$$即$$\\hat T_{ij} = C_{ik}C_{js}T_{ij}$$本质都是因为拥有类似于$$\\hat T_{ij} = (\\hat e_i \\cdot T)\\cdot \\hat e_j$$的表达式，在两种情境下都有鲜明的物理意义。"},{"title":"弹性力学-02-应变分析和应力分析","date":"2022-03-13T13:19:43.000Z","url":"/2022/03/13/post11/","tags":[["弹性力学","/tags/%E5%BC%B9%E6%80%A7%E5%8A%9B%E5%AD%A6/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["我来我见我征服","/categories/%E6%88%91%E6%9D%A5%E6%88%91%E8%A7%81%E6%88%91%E5%BE%81%E6%9C%8D/"]],"content":"应变分析知识框架 几何方程：从位移场出发，对位移场的微元做亥姆霍次分解，得到柯西应变张量。 变形：考察线元长度平方的变化，得到格林应变张量；分别分析三个主方向上长度变化和主方向夹角变化，得到柯西应变张量各个分量的物理意义。需要注意，$\\gamma$是夹角变化值的一半。 坐标变换：合同变换，$\\Gamma ^{‘} = C \\Gamma C^T$;考察坐标变换下不变量，这里是纯代数内容，三个不变量；由应变张量对称性，知必定存在三个互相垂直的主方向，此时剪应力为0。 协调方程：$\\Gamma$是由位移场微分得到的，6个分量里只有3个是相互独立的，所以一定能提出约束条件 volterra积分：这是一个有价值的反问题。volterra积分的目的在于根据原始的微分式找出积分，积分对象是应变张量的函数，这么做是因为方便。 位移场与流体力学最不同的是，弹性力学研究物体变形关注的是变形前后物质点的位移，有点拉格朗日观点的味道。事实上给出的位移场的含义是空间中某一点变形后的位置相对于原先点的位移，是空间场。能这么做的原因在于前文提到的弹性假设，每次卸载后弹性体都可以回到原位置，所以总是可以确定下一个所谓的初始位置的。$$r^{‘} = r+u$$ 这里的$u$即位移场。 几何方程就是非常非常径直的泰勒展开…$$u(r+dr)=u(r)+(u\\nabla)\\cdot dr$$$u\\nabla$是我们关心的东西，它可以做对称与反对称部分的展开。对称部分即应变张量$T$。 这个东西可以描述任意小的线元（$dr$）变形后的样子（$(u\\nabla)\\cdot dr$），也就是包含了所有这一点上变形的信息。 和流体力学变形张量不同的是，那里研究的是流体速度梯度，但一样的是都需要关注挨的足够近的质点。事实上速度与位移只差一个时间，并没有本质上区别；只是对于弹性力学问题没有时间的提法（其实也不一定？可能更多因为弹性力学问题中容易测量得到的是变形，而流体力学中相对容易测得的是速度）。 应变协调方程我们可以从位移得到变形，这是几何方程干的事。 这里有一个非常有价值的反问题，我们是否可以从变形（应变）还原得到位移场？ 答案是可以的。考虑现实情况，变形不能用位错等奇奇怪怪的情况，变形显然不能是任意的，我们需要对变形张量提出限制。$$\\nabla \\times \\Gamma \\times \\nabla = 0$$这是因为假如可以还原到位移场，那么$\\Gamma$来自于$u$的梯度，大致可以理解为有势，所以就可以提这样的应变协调条件。 在直角坐标系里可以把这件事看的更清楚。（哦下面写错成应力张量的分量了！是应变张量的分量，该用$\\epsilon$，$\\gamma$表示）$$\\sigma_x = \\frac{\\partial u}{\\partial x},\\sigma_y =\\frac{\\partial v}{\\partial y}, \\tau_{xy}=\\frac{\\partial u}{\\partial y}+\\frac{\\partial v}{\\partial x}$$这三个量是由$u,v$两个量表示的，所以它们之间一定满足某种关系，可以通过求导看清这件事，一定会存在这样一个关系$$\\frac{\\partial^2 \\sigma_x}{\\partial y^2}+\\frac{\\partial^2 \\sigma_y}{\\partial x^2} = \\frac{\\partial^2 \\tau_{xy}}{\\partial x \\partial y}$$所以也可以发现，那三个应变分量是由一个位移场表示而来的必要条件是这个协调方程。想积分有位移场，就一定满足协调方程。 Volterra积分表示就是说，知道了应变场可不可以积找出位移场。 这玩意太平凡了。。就是积分，然后化简一下。 大意是位移场的积分可以做亥姆霍次分解，把含反对称部分的项分部积分，把全微分项扔出积分，里面就有$d \\omega_{\\rho} = d \\rho \\cdot \\nabla \\omega_{\\rho}$，这件事是好的，最开始反对称项里是$\\omega_{\\rho} \\times d \\rho$，$d\\rho$是以叉乘作用，没法齐整的提出来。 这样转化为了点乘，就可以齐整的把积分表示成$d\\rho$乘仅含$\\Gamma$式子的积分，这件事是容易做的。 值得关注的是$$\\nabla u =-\\Omega+\\Gamma$$ $$u\\nabla = \\Omega+\\Gamma$$ $$\\Gamma \\times \\nabla = \\frac{1}{2}(u\\nabla+\\nabla)\\times \\nabla = \\frac{1}{2}\\nabla(u \\times \\nabla) =-\\nabla\\omega$$ 体积变化率这是一件有意思的事。 格林应变张量这一派关心微元变形前后长度的变化。 所以其实下图干的事情就是紧凑的表示$(d \\widetilde r)^2-(dr)^2$，$\\xi$是$dr$线元的方向。 所以某方向$\\xi$上的应变率为（小变形时）$$\\epsilon = \\xi_i \\gamma_{ij}\\xi_j$$ 应力分析知识框架 定义了应力，体力 由应力定义可以引出应力张量分量变化公式，再是纯代数的内容，推导出不变量、最大剪应力等结论。 由受力平衡推导出平衡方程，力矩平衡推导出应力张量对称。 应力和体力应力是内力，是体内微元之间的作用力。某个面上，单位面积上所受作用力的集度，与压强同量纲。 体力是外力，比如重力，定义为$$f = \\lim\\limits_{\\Delta V\\rightarrow 0} \\frac{a(\\Delta V)}{\\Delta V}$$常见的是重力场下的体力，比重。 应力张量这里的想法是自然的。 取一个立方微元体，每个面都有力作用，在面上对力可以正交分解为三个方向：法向，面上两个互相垂直的方向。但是面的法向又有三种情况。 所以在直角坐标系下，想精确的确定一个力需要9个维度的信息，每个维度都有面的法向（3维）和面上力的取向（3维）共同构成。 这就是应力张量$T$,它的各个分量甫一提出就可以看到鲜明的物理意义。 $\\sigma_i$代表$T_{ii}$，为某个面上与法向共线的力，也就是正应力。 $\\tau {ij}$代表$T{ij}$，是法向为$i$的面上，与$j$同向的力，此时$i,j$不互等，为剪应力。 任意方向上的内力$$n \\cdot T = t$$ 这里的n是方向。这个式子的证明利用了正四面体的平衡条件。直观的说，就是把$t$分解到直角坐标系的三个方向分别平衡。 平衡方程取正六面体，平衡时，应有合外力为0，即应力和体力之和为0.$$\\int {\\partial \\Omega}Tds+\\int{\\Omega}fd\\tau = 0$$利用奥高公式，得$$\\nabla \\cdot T + f =0$$对于力矩平衡，也可类似地提出方程。证明过程中的要点是，尽力转化出同类型并合并，利用平衡方程可证得应力张量的对称性。 应力函数这里的意思是，想求无外力时的应力，即$$\\nabla \\cdot T =0$$的解，发现这种情况应力的分量可以由几个任意的“足够好”的函数的导数表示。得到Beltrami-Scharfer解$$T=\\nabla \\times \\Phi \\times \\nabla+h\\nabla+\\nabla h-I\\nabla \\cdot h$$其中$\\Phi$是对称张量场，$h$是调和矢量场。 其实也挺平凡的，可以看成是一种张量分解。 坐标变换同样满足合同变换$$CTC^T$$正应力和最大剪应力理论都是纯纯的代数内容。"},{"title":"机器学习-01-ode inspired NN architecture design","date":"2022-03-09T07:39:47.000Z","url":"/2022/03/09/post10/","tags":[["machine learning","/tags/machine-learning/"],["动力系统","/tags/%E5%8A%A8%E5%8A%9B%E7%B3%BB%E7%BB%9F/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"应用数学讨论班的第三节课，董彬老师讲了他在机器学习方面的一些工作。但我只抓住了ode离散格式启发神经网络架构设计的insight，就记录在这里，当然不会很全面，想深入了解还是得读文献。 深度神经网络以全连接网络为例，输入$x$，每一层网络的运算都相当于$Ax+b$，把$x$加个维度，数值是1，就可以紧凑的写成$A \\hat x$，再考虑激活函数，我们一般性的，把神经网络每一层都看做一个单层感知机，接受的输入是上一层的输出，如果我们关心每一层间发生的事情，就可以把正向传播写成：$$x^{k+1}=f(x^{k},\\theta)$$$\\theta$是网络参数。传统的网络可以预料的会出现梯度消失梯度爆炸的问题，因为本质就是$$y = f^n\\circ f^{n-1}\\circ …f^{2} \\circ f^1(x)$$函数层层复合，层层迭代后李雅普诺夫指数就爆炸了（好像是这么说？） ResnetResnet可以显著的缓解这个问题，让梯度消失梯度爆炸不再制约神经网络的加深。我们还是关心神经网络每一层间发生的事，resnet做的事情其实就是$$x^{k+1}=x^k+f(x^{k},\\theta)$$把输入加到这一层的输出，一起作为下一层的输入。 到这里，神经网络和离散动力系统的关系就很明晰了。 ode inspired于是想到，ode的离散求解格式千千万，比如explicit euler,RK-form，可以不可以设计神经网络，使得他们每一层发生的事都满足那些离散格式呢？答案是可以的。最简单的为例：$$x^{k+1}=x^k+f(x^{k},\\theta)\\Delta \\eta$$这里的$\\Delta \\eta$就是离散格式里的步长(step size)，回到神经网络架构，就是下一层的输入里包含本层输入和乘上权重的本层输出，调整步长也就是调整了神经网络架构。需知这里的step size并不是一个像learning rate那样的超参数，改变它意味着改变了网络架构。 当然好的工作采用了更复杂的离散格式，引入了加速算法（松弛因子啊什么的），果然可以做到引入离散格式架构的网络在较浅的层数下就可以达到一般较深网络的优化效果。 总结 ode inspired架构设计不关心ode，关系的是怎么根据ode离散格式设计网络架构，使具有更好的优化效率。 ode离散格式里同样可以引入优化算法，但这个优化算法和神经网络优化算法（如admm）不是一回事，比如引入Nesterov加速算法，是为了找到对应具有更优效率的神经网络架构，和求解神经网络参数的gradient descent优化算法不是一回事。 董老师上课还讲到把离散格式连续化会对应成一个最优控制（optimal control）问题，但并没有搞懂。“从离散到连续可以采取很多方式”，不是很理解这句话。 鄂院讲到有篇工作是正儿八经的把神经网络转化成最优控制问题，不是传统的gradient descent，董老师介绍的这些工作其实本质还是反向传播神经网络，但用ode启发架构设计，而这些离散动力系统可以与一个最优控制问题相对应，并不是说把这些网络都转化为了最优控制问题。 下期预告董老师还讲了pde net，但并没有听懂，之后找文献读一读争取看懂吧。"},{"title":"并行计算-01-当代处理器与并行","date":"2022-03-07T11:17:02.000Z","url":"/2022/03/07/post9/","tags":[["硬件","/tags/%E7%A1%AC%E4%BB%B6/"],["并行计算","/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"]],"categories":[["我来我见我征服","/categories/%E6%88%91%E6%9D%A5%E6%88%91%E8%A7%81%E6%88%91%E5%BE%81%E6%9C%8D/"]],"content":"本文的组织结构总体是讲一个硬件知识，讲一组对应的高性能优化方式。毕竟硬件是一切高性能优化手段的根源。 Von Neumann架构当代计算机的机构基本都可以归结为冯诺伊曼架构，实现并行计算、高性能计算的落脚点也在架构本身。 组成部分 控制单元：解释指令（译码） 处理单元：执行指令 内存（memory unit，翻译成内存似不妥）：存储数据与指令（指令也以数据的形式存储，高速缓存中指令与数据不显示区分） I/O：与外界交互 控制单元与处理单元组成CPU，与内存（memory unit）通信。 指令处理数据是计算机完成任务的单元。 高性能策略分类针对指令简化指令CISC到RISC。 指令级并行superscalar(超标量)同时译码多个指令。 流水线之后会详细讲，这可能也是最常见的指令级并行手段 针对数据数据级并行，单指令处理多数据，即向量化、阵列化。 福林分类 从数据流、指令流两个维度进行分类。 最常见的应该是单指令多数据的并行方式。事实上大部分指令级并行都集成在硬件上，能人为在程序实现的一般也都是数据级并行。 流水线指令执行方式 从寄存器（register，最快的一块存储）中取指 控制单元译码 执行 写回到寄存器 一般来说，一个时刻每个阶段都只能有一个指令正在进行（超标量就是可以同时译码多个指令），由计算机的不同unit分别完成。流水线就是要让这几个unit都充分发挥作用。 定义 如一个任务由五条指令顺序完成，每条指令执行有四个阶段，就可以流水如图，如果等每条指令四个阶段都完成了再执行下一条指令，效率会大大降低。 流水线的超集是脉动阵列。 为什么可以流水数据不依赖流水其实不是一件平凡的事。第二条指令可以在第一条指令还没结束就开始译码，需要保证这两条指令相互独立。如果说第二条指令需要用到第一条指令处理的某个数据，那在第一条指令完成前就开始译码显然是行不通的。 流水的限制 更多相互独立的指令可以加深流水，提高效率，但这不现实。 分支预测错误。上面我们提到了数据不依赖，但在条件判断语句中，条件判断完成之前不与后面的语句独立的。为了能在这个地方也流水，大部分CPU都集成了分支预测功能。即在条件判断完成前先找一种最可能的情况往下跑，但假如预测错误就得全部倒带回去重跑。这个代价是比较高昂的。 存储技术冯诺依曼架构的缺陷前端总线（FSB）连接内存和处理器，它的延迟和带宽极大限制了性能。 存储的速度越来越落后于处理的速度（摩尔定律），这催生了多级存储技术。 多级存储技术越快的存储芯片，越因为发热量和造价，造不大。所以有寄存器到缓存到内存到外存的多级存储技术。 缓存命中访问数据时会从寄存器到各级缓存中查找，假如在缓存中就称命中，不然则失效。 多级缓存的合理性 加速手段数据越是在高速缓存里，程序跑的越快。但这不能由程序员控制。只能通过对存储架构的理解，用特定的写法、编程技巧来暗示编译器把他们想要的数据放进高速缓存。 缓存线是数据进入、清出缓存的最小单位。 数据局部性缓存线的是有限的。就是说如果你的数据都挨一块，那有一个数据进了缓存线那么一坨数据都进了缓存线；假如需要的数据存的很散，那可能触发了数据进入缓存线，仅有一小部分数据真正进入，其余混在缓存线进去的数据都是用不到的。 这就是所谓的局部性越高，数据进入缓存的机会越大。（确实很形象但这种说法总感觉不太舒服😂和大部分力学数学教材的说法不是同一种风格） 映射关联数据进入缓存的一种方案是根据数据地址按12取模进入对应的缓存线。有一种缓存失效为冲突失效，不同数据被映射到同一块缓存，解决这个方案有多路关联。即缓存线多个一组与内存数据映射。 这种做的好处在于，可能一段时间里数据按12取模的值的分布不是均匀的，比如模4的多，模8的少；如果单路关联，模4的缓存线冲突了而模8的甚至没完全利用，通过多路关联将这两路合为一组，模4的模8的都进入这一组缓存线，就可以提高缓存线利用效率。也就是增强了容错能力。"},{"title":"流体力学-02-重新审视基本方程组","date":"2022-03-07T07:22:07.000Z","url":"/2022/03/07/post8/","tags":[["流体力学","/tags/%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["我来我见我征服","/categories/%E6%88%91%E6%9D%A5%E6%88%91%E8%A7%81%E6%88%91%E5%BE%81%E6%9C%8D/"]],"content":"上学期流体力学基本方程组是一点没学明白，全部力气都花在看懂数学细节上了，并没有对这块形成比较成体系的认知，现在把它补回来。 连续介质力学方程组紧凑的表达 $$ \\left\\{ \\begin{aligned} & \\partial_t \\rho+\\nabla \\cdot J_{\\rho} =0 \\\\ & \\partial_t \\rho v+\\nabla \\cdot J_{ v} =0 \\\\ & \\partial_t E+\\nabla \\cdot J_{ E} =0 \\end{aligned} \\right. $$ 分别代表质量守恒、动量定理、能量守恒。$J$分别代表质量流、动量流、能量流。 这意味着可以把流体力学纳入之前学习的力学体系里，上学期悟性欠缺做不到这一点sigh 事实上对大部分连续介质问题都可以有一样的提法，其中本构关系决定了研究的具体是什么问题。对于流体力学的本构关系提法中最重要的假设就是偏应力分量与应变分量成线性关系，事实上线性化也是研究大部分连续介质问题会做简化的处理。 连续介质问题的一般提法知道自己所学的东西在整个学科体系里的位置是有益的。 古典流体力学（不包括湍流）的研究思路都是对流体引入连续介质假设，就可以用场论的手法去处理流体问题——可微了嘛！事实上包括弹性力学、电动力学在内都可以包入连续介质问题的范畴内，可以根据各种更基本的原理拿出方程、本构关系，但数学模型是类似的；比如说流体力学中的涡量感生的数学模型和电磁学中磁场问题是一模一样的。不过电动力学和流体力学、弹性力学不太一样，它的力场更复杂些，也可以提出更复杂的本构。 事实上场论处理的是欧拉观点的下的问题，要处理拉格朗日观点下的问题，就用的是分析手段，似乎也引入很多物理intuition(?) 比如我印象非常深的一个连续性方程的提法，由奥高定理$$\\nabla \\cdot v = \\frac{\\oint_s v_n ds}{\\delta V}$$曲面积分的积分区域取为物质体微元，所以$\\oint_s v_n ds$是物质体体积变化量，故$\\nabla \\cdot v $是物质微元体积变化率，当流体不可压时速度散度就为0. 连续性方程的提法是非常一般的。这里我们关注质量守恒，所以考察的物理量是速度$v$，如果是电荷守恒那就另说了。 流体力学方程组的提法把脉络梳理一遍。 应变张量从$$v = v_0 + \\frac{\\partial v}{\\partial x_i}\\delta x_i$$到$$v_i = v_{0i}+ a_{ij}\\delta x_{j}+S_{ij}\\delta x_j$$再到亥姆霍兹分解$$v=v_0+\\frac{1}{2}rotv \\times\\delta r+grad\\phi$$其中$$\\phi = \\frac{1}{2}\\delta r \\cdot S \\cdot \\delta r$$其实并没有提供什么多余的信息，只是把物理量用更紧凑更具有物理意义的形式写出来了而已，纯纯的场论分析手法，没另外提供什么物理图像。 然后分析应变张量的物理意义，这是不平凡的；发现$$\\frac{d}{dt}\\delta r=\\frac{\\delta v}{\\delta x_i}\\delta x_i$$捯饬来捯饬去就可以构建起速度梯度与长度变化量之间的关系。上学期根本就没把这件事看清楚，现在大抵可以做到独立的从头把这件事明白了。 基本方程连续性方程、动量方程和能量方程都是关注物质微元，根据基本原理非常直接的就能写出来。 对于动量矩方程，意义在于发现应力张量分量是对称的，相当于将九个未知量减少到六个，但此时方程组仍然不封闭。 本构关系为了让方程封闭，需要引入物性的知识，也就是本构关系。 这里最核心的insight就是假设偏应力张量分量和速度梯度分量存在线性关系。 改写方程这样我们得到的方程里包含应力张量，这不好，因为实际上我们很难直接得到应力张量，实验很难测的，所以要改写为矢量式，看起来更复杂了但实际上更好处理了，物理意义也更加明晰。 对于粘性不可压流体就得到了大名鼎鼎的N-S方程： $$ \\left\\{ \\begin{aligned} & \\nabla \\cdot v = 0 \\\\ & \\rho \\frac{d}{dt}v=\\rho F-\\nabla p+\\mu \\Delta v \\\\ & P = -pI+2\\mu S \\end{aligned} \\right. $$ 兰姆-葛罗米柯方程这个方程从动量方程来的，把随体导数变为就地导数后做了位势和涡旋的分解$$\\rho(\\frac{\\partial v}{\\partial t}+\\nabla \\frac{V^2}{2}+(\\nabla \\times v) \\times v)=\\rho F+\\nabla \\cdot P$$非常有用啊，所以单独注记下。"},{"title":"张量-02-协变逆变","date":"2022-03-07T06:45:28.000Z","url":"/2022/03/07/post7/","tags":[["张量","/tags/%E5%BC%A0%E9%87%8F/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"《连续介质力学》毫无疑问这是一本好书啊。李植老师的译笔非常流畅清晰。 当时在拼多多上花十多块买的，到了一看发现还有同济大学图书馆的章子，可给我吓一跳。也不知道是怎么从同济大学图书馆流到拼多多卖家上的，二手书市场有点意思哈。 07年时候我涛哥还是博士生，年代感一下子就出来了。 去年这时候压根就没读懂这本书，事实上这本书也不是写给新手入门，“为后续流体力学弹性力学学习打下基础的”。这本书的观点极高，讲的很数学很严谨，更适合在学习过流体力学弹性力学对张量运算、古典场论有了一定程度认知后，学习这本书进行提纲挈领式的拔高。 毕竟流体力学、弹性力学都属于连续介质力学的范畴，对物理对象引入连续介质假设建模，就可以用古典场论的手法分析。而连续介质力学是七大物理模型之一（量子多体、DFT、MD、CGMD、连续介质力学、kinetic theory（玻尔茨曼方法）、湍流模型），有了相当的基础之后学习这本书可以了解自己所学在整个科学体系里处于一个什么样的位置，以通向进一步学习。 所以说这本书真的不是给本科低年级学生打基础用的！！！！！！ 斜边分量逆变分量协变性这是一个相当物理的概念。是说一个物理量如果不随着坐标系改变而改变，那么它具有协变性。比如一个棍子你不管从哪个角度去看它，顶多影响你看到的形状，却改变不了它还是那个棍子的事实。但对于坐标基矢量这显然不满足。 一切的推导都从张量是具有协变性的量开始。 协变分量逆变分量。。。说真的我当时就觉得这是一件非常无聊的事，真没有硬分出一个谁协变谁逆变出来，这本身就是一个互反的概念，有了协变就一定会相应的有逆变，反之亦然。 但我当时还没办法把这件事看的很清楚，现在可以把我的理解说出来了。 一年级学过线性代数的同学就可以把这件事想明白。 假如两个坐标系的基矢量在笛卡尔系中分别由两个矩阵$A$ $\\hat A$表示，存在变换矩阵$$\\hat A = CA$$那么假如原先有一个矩阵在坐标系$A$中表示为B，那么在坐标系$\\hat A$中表示为$$C^{-1}B$$协变逆变就是这个道理。 写成张量形式，若基矢量间有变换公式$$c_{ij}\\hat e_j = e_i$$若有$$[A] = A_{ij}e_ie_j$$那么在$\\hat e_i$为基矢量表示下$$[A]=A_{ij}c_{ip}\\hat e_pc_{jq}\\hat e_q = A_{ij}e_ie_j$$得到分量变换公式$$\\hat A_{pq} = A_{ij}c_{ip}c_{jq}$$坐标变换系数很自然的换了位置。 本质就是我们前面提到的分量跟着基矢量跑，没那么多复杂的协变逆变的规则，混变就更简单了，部分基矢量变换部分基矢量不变。 张量记法本来就只是一种简记的notation，没那么多深奥的学问，一两句话可以说清楚的事引入一堆名词、规则，反而入了下乘。 张量这一Part可能短时间内不会更新了。"},{"title":"弹性力学-01-基本假设","date":"2022-03-01T11:51:36.000Z","url":"/2022/03/01/post6/","tags":[["弹性力学","/tags/%E5%BC%B9%E6%80%A7%E5%8A%9B%E5%AD%A6/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["我来我见我征服","/categories/%E6%88%91%E6%9D%A5%E6%88%91%E8%A7%81%E6%88%91%E5%BE%81%E6%9C%8D/"]],"content":"基本假设弹性承载变形， 卸载则恢复。 与之相对的是塑性，比如橡皮泥能给捏来捏去。弹性假设的意义在于，研究的对象在每次实验结束后保持性质不变。 代表性体元微观足够大，宏观足够小。与流体力学中流体微团的概念相似。 是连续介质力学里最重要的想法，意义在于： 微观足够大：原子级别的改变不影响微团性质，其实保证了均匀，包含有足够多的固体微观结构；具有固体性质的代表性。 宏观足够大：可以运用分析里的求导积分工具。 均匀连续均匀的概念在上面介绍过，固体里每一处体元都包含有足够多的固体微观结构；具有固体性质的代表性。 连续意味着物理量函数可以求导，忘记在哪看到了，是可以保证三阶导及以上连续，这是一个非常强的假设。 小变形小变形意味着线性性。应变和应力的函数关系可能很复杂，但在小变形下它就是线性关系。 很好理解——泰勒展开取一阶小量，忽略高阶小量。这是一个i合情合理的假设，弹性力学研究的问题也不会涉及到大变形。事实上在一定尺度内的变形，应力和应变就是线性关系，这是内部结构还没收到破坏。 无初应力这个假设比较迷惑，一时间没搞懂它的意义。是想说有初应力就一定可以找到无应力的零状态进行研究吗？ 用无初应力假设，是为了干净的提出正比的本构关系。 名词解释应力沿物体内部截面单位面积所受的力。 变形体内一点处（可由该处代表体元表达）的应力状态：该代表体元对应截面上单位面积上的力， 可分解为法向力（正应力）和切向力（剪应力）。 应变沿物体内部某方向单位线素的伸长和相对转动。 变形体内一点处（代表体元） 的应变状态：该代表体元沿某方向单位长度的伸长（正应变）和 体元边长夹角的变化（剪应变）。 应力和应变都具有位置和方向的两重性。 代表性体元相对于“微观”，它的尺度为无限大（包含有足够 多的固体微观结构；具有固体性质的代表性）；相对于“宏观” 固体，其尺度为无限小（采用微积分时对应微元尺度；是微积分 概念的一个完美的延伸）。"},{"title":"流体力学-01-量纲分析与相似律","date":"2022-02-28T13:38:39.000Z","url":"/2022/02/28/post5/","tags":[["流体力学","/tags/%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["我来我见我征服","/categories/%E6%88%91%E6%9D%A5%E6%88%91%E8%A7%81%E6%88%91%E5%BE%81%E6%9C%8D/"]],"content":"量纲分析基本量任何物理量都涉及最多七个物理量，力学问题一般只涉及三个（质量、长度、时间，被称为LMT问题）。 物理量名称 量纲 单位 质量 M kg 长度 L m 时间 T s 热力学温度 $\\theta$ K 电流 I A 光强 J cd 物质的量 N mol 量纲要学习$\\pi$定理得熟悉量纲写法。 我们比较熟悉的是单位，如表面张力$\\sigma_{s}$单位是$kg \\cdot s^{-2}$，它的量纲是$$dim \\ \\sigma_{s}= dim\\ \\frac{力}{长度} = M\\cdot T^{-2}$$为了之后理解幂次率，先说说单位是什么。比如我们说一个物理4$kg$重，就是说它的重量等于4个$1kg$。 好像说了句废话，但其实意思就是，我们在衡量一个量大小的时候，始终都是在和一个标准量作比较，这里的标准量就是国际标准单位制。我们关心它是几倍的标准量，即关注它的scale。通过这样一个标准化的过程，我们得以把各种问题都放在仅有scale的大框架里考虑。 听起来是不是有点像线性空间里取单位正交基？ 那么标准量只有一种取法吗？从这里延伸出了之后的故事。 量纲相关定律关于量纲有几个看起来很平凡，但其实证明起来不太省劲的定理。 任何物理量量纲都是基本物理量的幂次单项式。 每个公式里相加的量其量纲必须一致。 理解了第一个就很好理解第二个，当运算中幂次具有线性性的时候，我们往往不关注和，而关注积。这里隐隐有窥见一般性规律的道路，事实上这两个定律分别对应Lie群的拉伸群和对称性，可惜笔者不会。 $\\pi$定理$\\pi$定理很神奇，它说：一个物理问题里有$n$个物理量，这$n$个物理量最少可以$m$个基本物理量表示，那么我们从这$n$个物理量里找出无法相互表示（独立）的$m$个物理量，从而剩下的$n-m$个物理量可以被表示成这$m$个物理量的某种组合，即无量纲化为$\\pi _{i}$，物理问题内蕴的规律就可以用仅含有无量纲参数的函数表示。 其实这本质上反应了线性代数里的一件事： 从n个向量里找出m个最大线性无关组，把这m个表达为1组基，事实上这组基一定能构成方针——$m$也是这个问题涉及的基本物理量的个数，相当于这个问题的标准正交基是$m \\times m$的（不关心其余基本物理量了），再找最大线性无关组也一定是$m \\times m$的。于是有了一件很平凡的事，剩下的物理量一定可以被唯一表示成这组基的线性组合。 这样一番操作看似没有给出更多信息，其实在应用该定理时，期望得到的形式是关注的因变量的无量纲化参数等于一个包含一系列无量纲参数的函数，如果恢复因变量的量纲，那就得到了因变量等于一系列物理量相乘的形式，系数是无量纲参数的函数，往往需要由实验或者物理知识分析确定。 但其实已经得到无量纲参数的关系式了，没必要把它拆回去，事实上无量纲参数的关系更能反映一个物理问题的本质。 无量纲参数附上常见无量纲参数。 相似性原理一个物理问题完全由它的控制方程，往往是一组pde决定，这些方程同样是物理量之间的关系。 假如有两个问题，它们的几何情况相似，即它们的几何特征在找到合适的尺度放缩后完全一致；也具有时空相似性，指四维空间里的点$（\\vec{r},t）$在找到对应的$R_i,Ti$标准化后，点点相同；也具有动力学相似，即任意相对应的力$\\vec{f_i}$，在找到对应的$\\vec F_i$标准化后，也可以处处相同，我们说这两个问题相似。（这里说的很不清楚，大部分书上都有这部分的详尽叙述） 相似有什么用呢？相似意味着将这两个问题用合适的特征量无量纲化后，得到的方程是完全一致的。 到这里可以解释为什么说更关心无量纲参数的关系，这反映了一个物理问题的本质。需要得到具体的量时，用特征量表示出来就行。这为一些大尺度问题的研究带来了思路：完全可以用对相似问题的研究来替代，得到无量纲参数的关系后一切自明。这事实上指导了实验探究。 口说无凭，到这里我可以把上学期热统学到的范德瓦尔斯方程串起来了。 范德瓦尔斯方程可以用平均场原理和粒子占体积的想法推导出来：$$(p+\\frac{N^2a}{V^2})(V-Nb)=N\\tau$$a,b的值决定了这个体系相变的性质。 定义critical point为P-V曲线一阶导二阶导都为0时$p,V,\\tau$的取值：$$p_c=\\frac{a}{27b^2},V_c=3Nb,\\tau_c=\\frac{8a}{27b}$$用这一组量将$p,V,\\tau$无量纲化，得到：$$(\\widehat p+\\frac{3}{\\widehat V^2})(\\widehat V-\\frac{1}{3})=\\frac{8}{3}\\widehat \\tau$$任何没有特殊作用力的气液体系相变都可以用这个方程描述，不管具体的体系a,b值。即这一系列问题都相似，搞明白了一个方程就搞明白了一堆方程👍我的评价是帅的不谈"},{"title":"博客建设日志","date":"2022-02-27T16:30:40.000Z","url":"/2022/02/28/post4/","tags":[["重大说明","/tags/%E9%87%8D%E5%A4%A7%E8%AF%B4%E6%98%8E/"]],"categories":[["去码头整点薯条","/categories/%E5%8E%BB%E7%A0%81%E5%A4%B4%E6%95%B4%E7%82%B9%E8%96%AF%E6%9D%A1/"]],"content":"02.28.22图片加载感谢这篇博客。一般能找到的教程只会说安装某插件，但其实markdown的里图片插入路径也得改，而且改了之后markdown页面无法正常显示图片，网页上才能正常显示。 数学公式需要在文章的front matter里加一行 03.09.22无法渲染多行公式原因在于hexo会把//转义为/。 使用如下提示，让公式按原有规则渲染。 "},{"title":"张量-01-运算法则与求和约定","date":"2022-02-27T12:29:40.000Z","url":"/2022/02/27/post3/","tags":[["张量","/tags/%E5%BC%A0%E9%87%8F/"],["连续介质力学","/tags/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6/"]],"categories":[["数理基础补完计划","/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"]],"content":"张量是如何爆杀我的大二下上李植老师的连续介质力学基础，用的是谢多夫那本书，一上来就用曲线坐标系，从协变性逆变性开始讲起，并且默认大家只消几道习题就可以熟练掌握爱因斯坦求和约定。彼时我还没锻炼出学业上的抗压能力，直接被斩于马下，迷迷糊糊听了半学期一节课都没跟上过，遂退课跑路:) 大三上杨延涛老师讲流体力学，用了吴望一老师的流体力学，张量是在笛卡尔坐标系里讲的，那个时候才算能用张量做一些计算，但一碰到对张量求导就卡壳。 这学期魏悦广院士教弹性力学，用了王敏中老师的弹性力学教程，这才对各种张量求导法则有了比较清晰的认知。 之后肯定会还会在各种地方碰到张量，协变性逆变性也许还会重新学一遍，这篇博文的内容就仅限于基础运算法则了。 张量是什么张量有一个非常物理非常漂亮的定义：在坐标变换下具有不变性的量（区别各向同性张量是各分量保持不变），即具有协变性的量。但很显然，这不是一个利于初学者快速掌握张量运算的定义:) 说的直白点，零阶张量就是标量，一阶张量就是矢量，二维张量就是矩阵，三维的张量就是三维矩阵etc.（事实上矩阵的表现形式只能说是张量的分量，比较本质的定义方式一定要用到坐标变换，这里不过多赘述） 例如三阶张量可以写成$A_{ijk}$，$ijk$三个指标就分别是三个维度上的索引，很自然的可以想到这也是三阶矩阵的形式。到这里就可以开始进入下一阶段了。 爱因斯坦求和约定（summation convention）首先不要把这个当成什么特别高大上的东西，这玩意只是个记号，就算是爱因斯坦提出来的它也只是一种记号(x) 先写一个矩阵矢量相乘的求和约定记法：$$A \\cdot a = A_{ij}a_j = B_i$$大家有注意到$j$成对出现了，这叫哑标，不具有特定的值，$i$单独出现，是自由标，是一个索引。 哑标的意义就是求和，如固定$i=1$，即我们关注$A \\cdot a$第一个分量$$A_{1j}a_j = A_{11}a_1 +A_{12}a_1+A_{13}a_3$$到这里大家就可以看的很明白了，爱因斯坦求和约定的作用在于简化表达式，把具有相同形式的一串待求和项紧凑的用哑标表示出来。 所以为什么说哑标不具有特定的值，因为哑标正是用于简写这种对特定指标求和的表达式，如在这个例子里，和式的每一项都具有$A$的列指标与$a$的指标相同的格式。从来成对出现的原因也在这里。 所以可以把爱因斯坦求和理解成用和式的通项公式紧凑的表示和式。 再来一个例子$$A_{iij} = A_{11j}+A_{22j}+A_{33j}$$更多的需要在各种例子里体会哑标的用法，接下来关于张量运算法则的说明也可以加深对爱因斯坦求和约定的理解。 张量乘法基矢想更好的理解张量乘法，一定要回到分量与基矢。 上面说到，$A_{ij}$等矩阵形式只是张量的分量，那张量到底是啥呢？ 完整的表示应该是这样，以二阶张量为例$$A=A_{ij}e_ie_j$$这样一个和式，就是完整的张量，就好像矢量里的$$a = a_ie_i$$ 这里有一个很难懂的概念，并矢$$e_ie_j$$$e_i$是这个坐标系的基矢，并矢其实就是把两个基矢拼起来了，有9种拼法，就是说完整表示一个二维张量需要9个不同维度的信息。 这里回到弹性力学，可以更好理解这件事。应该放一个弹性力学Note的链接，但还没写，之后补上。 弹性力学里我们考察体元的一个面上的某一个应力，面有三个法向朝向，面上应力分解出来同样有三个朝向。那么我想确定一个“基应力”的完整状态，我需要同时给出它所在的面和它自己的朝向——是不是和并矢的概念有异曲同工之妙。这样得到的基应力有9个，构成了一个二维张量：应力矩阵。 双乘直接上双乘了，一些基本的运算规则不在本博文里叙述。这里放了王敏中先生书上内容。 千万记住，分量的指标是跟着基矢的指标走的，比如A点叉B，是基矢$e_ie_j$和$e_ke_s$先点后叉，服从基矢量由内到外的规则，相应的，分量$A_{ij}B_{ks}$中的$jk$和$is$分别先后参与了点乘和叉乘（在考虑某一对指标的时候忽视其余指标就行，把分量with其余指标当成整体，如考虑点乘时，把$A_i$当成整体），得到的基矢量从右到左写（如双叉乘是最后得到了$e_pe_q$，说明乘在前面的pq位置上的分量，如果写反了就成了转置了）。（这一部分写的好糟糕…希望时间一长自己还能读懂） 运用基矢量是为了方便看清楚每一个分量指标参与的运算，如果熟练了是完全可以不用的。新手比较头疼的是做叉乘时置换记号下的三个分量不易写出，运用基矢量就能看的很清楚了。 补充下转置$$A = A_{ij}e_ie_j,A^{T} = A_{ji}e_ie_j$$是不一样的。 张量求导逗号割index &amp; variable王敏中先生的一个记法是，下标中，逗号后面的指标代表求导的位置。这个记法非常好的一点就是可以非常清楚的看出一个分量的位置（索引）和对哪些变量求导。 如$$A_{ij,ks}$$说明这是给对$x_s,x_k$求了二阶导的$A_{ij}$分量，一个逗号就起到了分割的作用，拎得很清。 求导法则 这里说的很清楚了，同样要记住分量跟着基矢量走，基矢量运算满足从里到外原则。 上流体的时候一直没搞明白矢量梯度张量梯度的含义，用基矢量去理解才弄明白。 散度也是一样，算子的基矢量在哪，求导就在哪。 注意事项到这里其实就应该差不多了，关键就是做运算的时候看紧基矢量的指标。再写一些自己会犯错的地方。 转置只转分量比如$$A \\times B = A_{ij}B_{ks}\\epsilon_{jkl}$$但$$A \\times B^{T} =A_{ij}B_{sk}\\epsilon_{jsl}$$是错的，这样同时转了分量与基矢量，相当于完全没转。 请看基矢量，正确写法应该是$$A \\times B^{T} =A_{ij}B_{sk}e_i(e_j \\times e_k)e_s = A \\times B^{T} =A_{ij}B_{sk}\\epsilon_{jkl}e_ie_le_s$$ 各项同性张量各阶各向同性张量的证明很有意思，是把坐标系旋转$2\\pi,\\pi, \\pi /2$等特殊角度，找到分量的规律，非常代数风格的证明。 张量加法$$A +B = A_{ij}+B_{ij}$$ 最好写成指标匹配的形式，以保证分量对应的基矢量一致。"},{"title":"Github-01-最最基本的操作","date":"2022-02-26T14:28:34.000Z","url":"/2022/02/26/post2/","tags":[["Github","/tags/Github/"]],"categories":[["去码头整点薯条","/categories/%E5%8E%BB%E7%A0%81%E5%A4%B4%E6%95%B4%E7%82%B9%E8%96%AF%E6%9D%A1/"]],"content":"之前创建了github账号，也关联了私钥，做了第一次commit&amp;push，但那次迷迷糊糊不知道为啥就传上去了。今天找了wrgg，才对基本流程了有了一点概念。 在这之前需要有什么有一个clash，安装了git，有一个github账号，在本地生成了ssh key，并在github账号中关联。因为我并不觉得这些操作会反复用到，一个key应该可以用到地老天荒(x) 对Git的粗浅理解我在接触git的时候会很好奇，是怎么把这个文件夹识别为仓库，分支什么的又是怎样记录，本地又怎样关联账号。 其实这一切的一切，都因为钝角(x)其实是一个隐藏文件’.git’。 大家都知道你要是在想本地创建一个文件夹把它变成github仓库，需要cd进去，运行git init指令。这个指令就是在目标文件夹里生成了.git，但你看不到，好像什么事也没发生，其实并不是。 很多时候我从哪随随便便copy来什么东西，想和自己的github关联，往往会出问题，就是因为.git文件标识了不同的账号。看不见并不带表不存在。 所以接下来一切操作都从干干净净的文件夹开始。 （提醒自己不要瞎搞，既然懂的不多就规规矩矩来QAQ） 把本地文件上传到github上假如要上传名为test的项目，可以分如下几步走： 建立一个干净的文件夹，git bash here 在github里创建一个名为test的repository，然后用git clone指令clone到空文件夹里，这时应该会出现一个名为test的文件夹 这样做不需要git init初始化仓库，因为clone下来的test文件夹里应该有.git了，已经可以被识别为一个仓库 把要上传的文件复制粘贴进test里 cd 进test git add .添加文件夹里所有文件 git commit -m ‘提交信息如first commit’ git push上传 到这里应该就搞定了。之后如果有啥修改也是一样的操作，替换增删文件，add commit push一套下来完事。 出现的问题无法clone属于是网络问题，bash里跑一句 就可以了。原理是让bash也可以接到clash。 但需要每次打开bash都跑一遍。 push了不显示用git status 看看到底提交没。多试试。 感觉怪怪的讲道理每次Push都要输入自己私钥的密码，但是wrgg不知道输入了什么，直接把我本地和github账号关联了，每次直接Push就传上去了。 算了能用就行…"},{"title":"00-欢迎","date":"2022-02-11T10:30:50.000Z","url":"/2022/02/11/post1/","tags":[["重大说明","/tags/%E9%87%8D%E5%A4%A7%E8%AF%B4%E6%98%8E/"]],"categories":[["这个男人来自地球","/categories/%E8%BF%99%E4%B8%AA%E7%94%B7%E4%BA%BA%E6%9D%A5%E8%87%AA%E5%9C%B0%E7%90%83/"]],"content":"05.09.22修订 1、增补了分类：每一步路都算数。 03.14.22修订 修改了分类：“我来我见我征服”的描述 03.13.22修订 1、全麻，看一次尴尬一次救命…掩耳盗铃式地调小了字体 2、增补了分类：“这个男人来自地球”的描述 02.26.22修订 1.好怪哦，才过了半个月再看这篇博文就尬的脚趾抠地，一股子酸酸的感觉迎面而来😥但不会做任何删改，毕竟这就是我当时的真实想法（但并不代表现在的想法x）确实是全麻，看一次尴尬一次救命 2.增加了一个category：去码头整点薯条 hello狮子大张嘴，pku19本，理论与应用力学系在读，爱好摇滚。 why blog?事实上很早就有搞个用来输出内容的平台的想法了。 大二时候零零碎碎地搞过一个公众号，因为各种原因不了了之（最主要的是想起的公众号名字居然都被用过了），大半年没发出一篇文章，干脆销号了。 21年22年之交，我在学工部门混到了一个位置，同时因为担忧学业，干脆决心退出部门，怀揣着也许漂亮但barely毫无用处的title，迷茫在这个世界四处张望居然找不到一个属于自己的位置。前两年因为个人习惯也因为糟糕的室友、繁忙的学工，想起来之前居然有即便是硬核专业课也全部翘掉、作业不写的荒谬经历。绩点命悬一线，荒废的学业都得一点点捡起来，只能埋头踏踏实实上课、做题。 也是在这个时候，我开始了我的第一段恋情。很快就结束了，但是余韵悠长。不破不立是对的，我重新思考我过去二十年对一些事物的认知，踟蹰着探索着自己的形状。 虽然前路上尚不明朗，但终于也知道了自己到底要往哪走。 这个博客，就是用来记录自己走过的路。也许是学过的东西，也许是想到的事情，也许真的只是一段路。 这个博客我会将它打造成一片个人的自留地，之后如果有强观点输出的要求，估计会重启公众号。还有就是公众号推文排版真的好麻烦（x “专注内容”“个人空间”，就是这个博客了，狮子大张嘴的博客。 how?目前计划有四个板块，分别是： 这个男人来自地球发表一些平时零零碎碎的想法、有意思的生活碎片，预计形式主要会是散文随笔。 这里或许会成为表达我态度的地方？预计开设板块[十六日谈]。 发表一些stank tone，或许将来看到以前自己foolish的发言会觉得非常有趣。 数理基础补完计划目的导向式学习的数理知识会记录在这里，同时计划把本科一二年级没学明白的东西再学一遍，也记录在这里。我的记性太差，有时候什么东西学明白过几天可能就忘了得再学一遍，做纸质笔记又容易乱扔，也是出于费曼学习法的要求，要输出！ 我来我见我征服一些学科笔记。不会有大量技术细节，只把主要脉络和重要的insight捕捉下来。一年后的目标是能够看懂“每种基本粒子都对应于一个庞加莱群的不可约表示”这句话。 去码头整点薯条平时零零碎碎学到的技能会发在这里，这些技能会时不时用到，每次用完了过段时间就忘了，记录下来以供自己查看，以免到处找资料。 每一步路都算数发表一些论文精度或泛读。标题的第一部分为论文所在领域。22年夏预计以阅读physics inform neural network（PINN）的相关论文为主。 welcome最后欢迎大家来我的博客玩！耶！"},{"title":"categories","date":"2022-02-11T10:06:28.000Z","url":"/categories/index.html","categories":[[" ",""]]},{"title":"tags","date":"2022-02-11T10:05:49.000Z","url":"/tags/index.html","categories":[[" ",""]]},{"title":"search","date":"2022-02-11T10:07:42.000Z","url":"/search/index.html","categories":[[" ",""]]}]